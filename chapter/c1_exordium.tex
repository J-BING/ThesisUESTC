\thesischapterexordium

\section{研究工作的背景与意义}

视觉问答（VQA）是近几年学界新兴的研究方向之一。视觉问答是一类输入为图像和用自然语言表达的文本问题，输出自然语言方式答案的人工智能任务。任务目标是构建一个像人类智能一样的问答系统——能够从给定的图片中，抽象凝结出图中物体的类别、空间关系、活动、场景等高阶信息；并根据问题的不同，针对性得给出合理的答案。

视觉问答主要涉及计算机视觉、自然语言处理、知识表达与推理三个领域。作为一个多学科交叉的领域，想实现高准确率的系统表现，既依托单个分支下理论、算法、应用系统的快速发展，作为其基础设施；同时还对各子系统的结合方式提出了很高的要求。正是由于视觉问答任务需要处理语言和图像两种重要的数据类型，这使得智能体更像人类一般思考和推理。智能体的“视觉系统”能够接收含有深层次信息的图像源；智能体的“神经系统”能解析图像信息和理解语言内涵；智能体的“语言系统”能够遣词造句，输出人类可理解的语言形式。因此视觉问答被认为是人类构建“人工智能完全体”的重要一步\citing{ antol2015vqa, malinowski2014towards, geman2015visual}。

视觉图灵测试\citing{geman2015visual}是一种能够衡量智能体是否在图像语义理解方面达到人类水平的测试方法,视觉问答任务被认为是智能系统通过视觉图灵测试的关键性技术。除了作为视觉图灵测试的核心部分，视觉问答还有其他具有价值的应用场景。a)作为盲人或是有视觉障碍问题的病患的辅助系统。通过自然语言询问，使用者能获得细粒度的图像或者视频信息，获得一种便利的“视觉补充”。b)扩充人机交互方式，在人机交互上可以实现多种的便利查询。通过对已有图像的询问，获得更深层次的背景知识，例如，对一副未曾见过的艺术名画询问其作者和作画背景，可以更深入的理解图像背后隐藏的人文和历史知识。通过源图像可以搜索具有相似“特征”的图像，例如，向系统查询一张埃菲尔铁塔的夜景图，将能获得更多具有相关特征的图像素材。同样可以通过图像描述查询到对应或者相似的图像。

总的来说，作为一个跨领域的人工智能任务，视觉问答的研究代表着对未来“通用人工智能”的探索，既能够提供一种跨模态的数据处理和融合方式，又能够向机器理解和解决复杂问题、甚至完成推理的人工智能新阶段迈进。

\section{视觉问答的国内外研究状况}
视觉问答任务具有广阔的应用场景和对人工智能发展的深远意义，自2014年VQA挑战以来，大量的视觉问答模型被提出。按照是否引入外源知识库，现有的VQA模型划分为两个大类：联合嵌入模型和基于知识库的模型。联合嵌入模型设计特征提取网络分别提取图像和文本特征，进行跨模态的特征融合，最后使用分类器预测答案，一般为端对端的网络。基于知识库的模型则通过引入外源知识库，获得额外特征和知识，试图克服联合嵌入模型数据集依赖等缺陷。其中根据知识库的引入方式的不同，基于知识库的模型又可以分为知识库查询类和知识库嵌入类。

\subsection{联合嵌入模型}
联合嵌入模型先将视觉信息和问题文本信息分别特征化，再通过特征向量串联\citing{zhou2015simple}、卷积\citing{ma2016learning}、逐元素相乘\citing{antol2015vqa}、逐元素相加\citing{malinowski2015ask}等方法融合图像特征和文本特征，最后使用分类器预测答案。

Malinowski等人首次提出了应用于真实场景的联合嵌入模型Neural-Image-QA
\citing{malinowski2015ask}。Neural-Image-QA使用卷积神经网络CNN提取图像特征，得到的特征向量和问题文本一起传输到长短期记忆LSTM中，生成答案的单词序列。模型在DAQUAR\citing{malinowski2014multi}数据集上的准确率为19.43\%。这种CNN+RNN的基本范式也被后来的研究者大量使用。

在文本特征化的方面，Zhou等人在处理问题文本时选择了比长短期记忆LSTM更为简单的词袋模型BOW，提出了iBOWIMG模型\citing{zhou2015simple}，并迁移预训练的GoogLeNet\citing{Szegedy_2015_CVPR}提取图像特征，在COCO-VQA数据集上的表现良好。Gao等人认为问题和答案在句法结构上有所不同，因此使用两个独立的LSTM网络编码问题和解码答案，并结合卷积神经网络构成了mQA模型\citing{NIPS2015_5641}。Lin等人既将卷积神经网络CNN应用于编码图像内容，也应用于问题文本的特征提取，并且使用一个多模态的卷积层输出联合特征向量，提出了双CNN模型\citing{ma2016learning}。

在图像特征提取方面，除了不同模型使用不同的预训练CNN外，Noh等人认为单一权重配置的深度卷积神经网络无法有效处理不同的问题\citing{noh2016image}。他们在卷积神经网络CNN中添加一个动态参数层，动态参数层中的参数会根据问题的不同而改变，这使得每个问题输入都对应一个独特的分类网络。提出的DPPnet模型由三个部分组成，作为分类网络的卷积神经网络、由门控复发单位构成的参数预测网络、将参数预测网络输出的动态参数配置到分类网络的哈希函数。

除了使用不同的方法提取图像和文本特征以外，跨模特征融合的方式也被大量研究。Malinowski等人通过对不同的特征向量融合方法的比较，证明了系统的准确率与特征向量融合方法有关\citing{malinowski2015ask}——不同方法之间准确率最多能相差9个百分点。除了以上提到的iBOWIMG采用向量拼接的方式，Lin使用向量卷积的方式外，Antol等人提出的模型使用逐元素相乘的特征融合方法\citing{antol2015vqa}。Saito等人认为不同的特征融合方法会保留不同层次的的特征，因此提出了一种逐元素相加和逐元素相乘相结合的模型DualNet\citing{saito2017dualnet}。Fukui等人认为向量之间的外乘运算中，所有元素之间的互动更加活跃，能保留更加丰富的特征信息，因此提出一种更为复杂的多模态紧凑双线性池化方法（MCB）\citing{fukui2016multimodal}。

由于注意力机制已经在大量的深度学习任务中被证明有效，视觉问答模型也广泛使用注意力机制。Chen等人最先将注意力机制引入视觉问答任务，提出了基于注意力机制的可配置卷积神经网络（ABC-CNN）。模型针对“图像问题对”生成对应的注意力映射，对问题和图像区域建立映射，使得答案生成取决于被关注区域，减少无关区域的影响\citing{chen2015abc}。在Toronto COCO-QA\citing{ren2015exploring}, DAQUAR\citing{ malinowski2014multi}, 和VQA\citing{antol2015vqa}三个数据集上的测试结果都实现了最优结果，证明了注意力机制在提高视觉问答任务上的有效性。

Shih等人使用CNN对图片的不同区域编码，根据图像特征和文本特征的点乘结果决定每个图像区域的权重，最后结合权重化以后的图像特征和文本特征得出答案。在辨别物体颜色的任务上得到了最优结果\citing{ shih2016look}。类似的工作还有Ilievski等人提出的“聚焦型动态注意力模型“\citing{ilievski2016focused}。

包括以上提到的在内，多数注意力机制对问题文本和图像区域特征进行一次运算，直接生成图像注意力权重图。而Yang等人则提出堆栈式注意力网络——使用问题的语义表达对图像进行多次查询，不断缩小答案相关区域，实现更高的精度\citing{yang2016stacked}。注意力机制在视觉问答上的其他应用还有，同时使用对图像和问题使用注意力机制的联合注意力模型\citing{NIPS2016_6202}；不采用图像区域赋值方法，而是过滤掉不相关区域的“自适应硬性注意力网络”\citing{malinowski2018learning}。

还有研究者希望存储部分训练信息供后续的迭代训练使用，从而在原有CNN+RNN结构基础上，引入了动态记忆网络\citing{jiang2015compositional,kumar2016ask,xiong2016dynamic}。Jiang等人在CNN+RNN的架构上，新增了一个成分记忆模块\citing{jiang2015compositional}，用于融合每一次训练过程中的局部图像信息和文本信息，并提供给下一次训练使用，从而使网络存储了训练过程的“经验”。Kumar等人为解决文本问答（Text-QA）任务而提出动态记忆网络（DMN）\citing{kumar2016ask}。动态记忆网络（DMN）是一个用于生成文本问题答案的神经网络框架，它由输入模块、问题模块、情节记忆模块和答案模块构成，问题模块用于编码文本问题；情节记忆模块接受由输入和问题模块得到的分布式向量，再使用注意力机制选择特征，结合选择后的向量与以往存储的“记忆”生成新的“记忆”向量，并不断迭代；答案模块根据最终的记忆向量生成答案。
动态记忆网络（DMN）在文本问答、语义分析、词性标注任务上取得了最优的结果。受到动态记忆网络的启发，Xiong等人在原有网络的基础上改善了输入和记忆模块，使其不仅能处理文本信息外，还能处理图像信息，因此提出了应用到于视觉问答任务动态记忆网络+（DMN+）\citing{xiong2016dynamic}。动态记忆网络+（DMN+）将原有的输入模块中处理文本编码的门控复发单元（GRU）更换为双向门控复发单元（bi-GRU）以得到文本或图像区域更完整的上下文信息；使用基于注意力机制的门控复发单元替换原有的软性注意力机制。该模型在DAQUAR\citing{malinowski2014multi}和VQA数据集\citing{antol2015vqa}上的测试结果都得到了具有竞争力的表现。

\subsection{基于外源知识库的视觉问答模型}
联合嵌入模型由于其灵活的结构和在通用数据集上的优异表现，成为了视觉问答任务中的主流模型，但是联合嵌入模型存在以下缺陷：

第一，数据集依赖。联合嵌入模型的答案生成来源于训练集中的问题和答案文本，这意味着训练集中包含的知识和文本内容是整个视觉问答系统的所有知识来源，因此对于测试集中的全新概念，模型很难得出正确的答案。不断扩充包含更多先验知识的训练集是提高精度的方式之一，但考虑到目前知识的巨大体量，这种数据集扩充的方式面临巨大的挑战。

第二，网络容量小。联合嵌入模型要求网络本身能存储学习到的知识，目前网络的容量相较于需要学习的知识是严重不足的。

第三，黑盒效应明显。对于识别和分类等问题而言，可解释性与高精确度相比，显得不那么重要，但是对于需要明确推理过程的问答系统而言，黑盒的不可解释性会降低提问者对系统的可信度。

为弥补数据集依赖和网络容量小的缺陷，研究者在联合嵌入模型的基础上引入外源知识库，提出了一些基于知识库的视觉问答模型。根据知识库的使用方式，基于知识库的视觉问答模型分为知识库查询类和知识库嵌入类。

知识库查询类的目标是根据图像和文本创建知识库查询语句，通过知识库查询获得答案。模型提取图片的实体、将实体映射到知识库、转化自然语言为查询语句、查询知识库。代表模型为Ahab\citing{wang2015explicit}和FVQA模型\citing{wang2017fvqa}。

Wang等人引入DBpedia知识库，提出了Ahab模型\citing{wang2015explicit}。Ahab分别使用预训练的Fast R-CNN\citing{ren2015faster}和两个不同的VGGnet\citing{simonyan2014very}从图像中提取物体对象、图像场景和图像属性三种视觉概念。所有提取出的图像信息都使用资源描述框架（RDF）的形式表示，例如，“图像中包含长颈鹿对象”被表示为（图像，包含，对象1），（对象1，名称，长颈鹿）。每个视觉概念则被直接链接到具有相同语义的知识库概念。在问题文本处理方面，Wang等基于自建的KB-VQA数据集——其中的问题需要常识或外源知识，设定了23种问题模板，将自然语言问题转化为相应的知识库查询语句，直接从知识库中查询得到答案。在KB-VQA数据集上，Ahab在每种问题类型上的准确率都远高于联合嵌入模型。

Ahab将问题解析为知识库查询语句时，需要预先确定问题模板，这极大的限制了模型能处理的问题类型，因此Wang等人改变了问题到查询语句的映射方式，提出了FVQA模型\citing{wang2017fvqa}。FVQA模型使用长短期记忆（LSTM）网络训练一个28类的查询语句分类器，实现将问题到查询语句的分类过程。在自建的FVQA数据集上和多个模型的对比结果显示，FVQA模型使用问题到查询映射模型能从问题文本中提取到关键信息，并能利用关键信息组成有意义的语言结构，再结合额外知识库搜
索到正确答案，并且可以反映出整个推理过程，实现了推理过程的去黑盒化。

另一类为知识库嵌入类，这种方式不用设计复杂的查询语句，而是将知识库的数据转化为额外的特征向量，并联合图像特征和问题特征一起训练。这种方式能省去问题模板和查询语句设计的人工成本，并且使得模型能应用于更大规模的开放型数据集，避免了自建数据集带来的训练和评估问题。为了提高视觉问答系统的问题的灵活性，Wu等人通过改进常见的CNN+LSTM的嵌入模型，提出了基于知识库的通用嵌入模型\citing{wu2016ask}。模型的基本架构由图像属性提取网络（CNN）、图像描述生成网络、外部知识库查询网络以及答案生成网络（LSTM）构成。该模型采用Toronto COCO-QA\citing{ren2015exploring}和VQA\citing{antol2015vqa}两个数据集进行评测，分别获得了69.73\%和55.96\%的准确率。

\subsection{前人工作总结}

联合嵌入模型是视觉问答任务的主流模型。大量模型探索了不同的特征提取方法、特征融合方法和注意力机制。总体来看，联合嵌入模型具有较很高的灵活性，并且在通用数据集上的表现也很好，例如在VQA挑战中2015-2019年的最优模型均是联合嵌入模型，因此其具有很高的研究价值。

注意力机制的引入能有效的提高模型的准确率。在引入注意力机制前，无论对于图像输入还是文本输入，特征提取网络将输入看做一个整体，提取的特征包含问题无关的信息，降低了模型的分类效果。引入注意力机制后，输入的编码方式改变，提取的特征是局部信息的综合，既包含了更多的重要信息，也减少了模型的无关运算，提高了执行效率。

通过引入知识库，基于知识库的模型能够改善联合嵌入模型数据集依赖和网络容量小的问题，相对于不引入知识库的联合嵌入模型，基于知识库的模型能提供图像和问题以外的信息，并且具有很高的知识存储容量。但其下的两类模型也各有优劣。知识库查询类模型通过人为设置查询语句，实现了在复杂推理任务上远远高于联合嵌入类模型的准确率。但也同样因为查询语句需要人为设计，当问题类型数量剧增时，人工成本骤增，这会大大限制分类数量和分类模型的精度。相对于知识库查询类的模型，知识库嵌入类的模型无需设计查询语句和构建数据集，因此可以使用通用数据集训练和评价。三种模型的比较可以简单描述为以下关系：
\begin{description}[labelindent=2em, leftmargin=6em, style=sameline]
\item [解决识别类问题：]知识库嵌入类>联合嵌入模型>知识库查询类
\item [解决推理类问题：]知识库查询类>知识库嵌入类>联合嵌入模型
\item [模型迁移能力：]知识库嵌入类=知识库查询类>联合嵌入模型
\end{description}

总而言之，联合嵌入模型由于其模块组合的灵活性，因此具有很高的改进空间。而知识库嵌入类的模型由于引入了知识库，能引入额外的特征，因此提高了其对推理问题的解决能力，有很好的研究前景，并且能够使用通用数据集训练，可实现端对端的训练。而知识库查询类的模型基于人为设计的查询模板，在推理问题上能实现更好的精度，但是查询模板和数据集的构建成本过高，限制了其进一步的发展。

\section{论文主要研究内容}

如上文提到的，目前的视觉问答模型可以划分为联合嵌入模型、知识库查询类模型、知识库嵌入类模型，本文重点研究了联合嵌入模型和知识库嵌入类模型。在已有工作优秀思想的基础上，考虑到模型现有的问题，本文分别对这两类模型进行了改进，从而提出了两个全新的模型。具体来说，本文的主要研究内容如下：

1. 视觉问答模型中的文本特征化方法的改进。通过对已有的联合嵌入模型的系统性研究，本文提炼出了视觉问答模型的基础架构，并且详细分解了一系列代表模型的结构。虽然目前的联合嵌入模型已经有较好的准确率，但其在文本特征化中仍使用静态词向量。考虑到真实预料中一词多义和一词多成分的情况，本文构建了一个biLSTM网络，用于学习场景化的词向量，引入Elmo动态词向量，提出了一个基于动态词向量的联合嵌入模型——None KB-Specific Network(N-KBSN)模型，并构建了一系列对比模型试验动态词向量和静态词向量对结果的影响。

2. 构建知识库图嵌入模块。为了提高基于知识库的模型的泛化能力，本文使用知识库图嵌入的方式引入知识库。为了得到优秀的知识库节点嵌入，本文进行了知识库嵌入实验。首先对原有的DBpedia知识库进行预处理，通过遴选数据子集、数据清洗、去URI化等步骤，构建了DBA和DBV两个实验知识库。随后使用TransE翻译模型在DBA和DBV两个实验集上进行了链路预测实验，通过对比实验，评估TransE模型在知识库嵌入的有效性，并以此构建知识库子图提取模块。

3. 基于知识库图嵌入的视觉问答模型。合并之前工作中构建的知识库图嵌入模块和N-KBSN模型，本文提出了基于知识库图嵌入的视觉问答模型——KB-Specific Network（KBSN）。知识库的图嵌入由子图提取模块和子图嵌入模块两个主要部分组成。知识库的图嵌入能表达实体之间的结构信息，从而增强特征的表达能力，并且低维的特征向量具有计算便利性，可以实现大规模的训练和预测，消除了人工设计查询语言的复杂性。最后通过在VQA2.0和KB-VQA数据集上的实验评估知识库图嵌入对模型的效果。

\section{本论文的结构安排}
本文的章节结构安排如下：

第一章，绪论。本章节主要介绍了视觉问答任务的研究内容和应用前景，对视觉问答的国内外研究状况作了归纳，其中重点介绍了已有的联合嵌入模型和基于知识库的模型，最后阐述和总结本文的研究内容。

第二章，视觉问答任务及架构基础。本章首先介绍视觉问答任务的基础知识，包括定义、问题类型和数据集。其中，按照答案与问题和图像的相关性，本章提出了一种新的问题分类标准；按照“是否需要知识”的维度，本章将现有的代表性数据集划分为基于视觉的数据集和基于知识的数据集，并做了概括性描述。本章还提出了视觉问答模型的基础架构，并分项介绍了其特征提取、注意力机制、特征融合、答案生成的方法、原理、公式等内容。

第三章，基于动态词向量的联合嵌入模型。本章首先分析了联合嵌入模型在视觉问答任务优异表现的原因，并且分析了现有模型的特点和局限。针对现有模型的局限，本文提出了基于动态词向量的联合嵌入模型——N-KBSN模型，随后详细介绍了N-KBSN模型的问题文本和图像特征提取模块、自注意力和引导注意力模块。最后构建了一系列对比模型进行视觉问答实验，根据实验结果，详细分析了动态词向量和静态词向量对模型准确率的影响，并解释了N-KBSN优异表现的原因，最后和已有模型进行了对比。

第四章，基于知识库图嵌入的视觉问答模型。本章简要介绍了知识库的发展历史，并且分析了几个重要的知识库各自的特点。本章提出了一个基于知识库图嵌入的视觉问答模型——KBSN模型，并详细介绍了本文构建的知识库图嵌入模块。随后通过知识库嵌入实验，验证了知识库嵌入的有效性和模型选取的合理性。最后本章在KB-VQA\citing{wang2015explicit}和VQA2.0数据集上训练和测试KBSN模型，通过对比其他模型，分析实验结果，证明了知识库图嵌入模块的有效性。

第五章，全文总结与展望。本章回顾了全文的研究内容和研究结论，明确了工作的创新点和贡献，分析了研究中的不足。最后对于存在的问题，提出了改进方法以及潜在的发展方向。

