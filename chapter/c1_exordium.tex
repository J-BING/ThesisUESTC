\thesischapterexordium

\section{研究工作的背景与意义}

视觉问答是近几年学界新兴研究的热门方向之一。得益于神经网络架构在自然语言处理和图像识别相关任务的成功应用，学界将研究的视线移向对系统智能要求更高的视觉问答任务。视觉问答任务是一类输入为图像和用自然语言表达的文本问题，输出为基于图像内容理解并且用自然语言方式呈现的答案的计算机视觉任务。简而言之，任务目标是构建一个像人类智能一样的问答系统——能够从给定的图片中，抽象凝结出图中物体的类别、空间关系、活动、场景等高阶信息；并根据问题的不同，针对性得给出合理的答案。

视觉问答主要涉及计算机视觉、自然语言处理、知识表达与推理三个领域。作为一个多学科交叉的领域，想实现高准确率的系统表现，既依托单个分支下理论、算法、应用系统的快速发展，作为其基础设施；同时还对各算法、子系统结合时的性能融合、体系结构提出了更多、更新的研究要求。这既是多学科高度融合状况下的挑战，也是令人兴奋不已的。正是由于视觉问答任务需要处理语言和图像两种重要的数据类型，这使得智能体更像人类一般思考和推理。智能体的“视觉系统”能够接收含有深层次信息的图像源；智能体的“神经系统”能解析图像信息和理解语言内涵；智能体的“语言系统”能够遣词造句，输出人类可理解的语言形式。因此视觉问答被认为是人类构建“人工智能完全体”的重要一步\citing{ antol2015vqa, malinowski2014towards, geman2015visual}。

视觉图灵测试\citing{geman2015visual}是一种能够衡量智能体系是否在图像语义理解方面达到人类水平的测试方法,视觉问答任务被认为是智能系统通过视觉图灵测试的关键性技术。除了作为图像理解的图灵测试的核心部分，视觉问答还有其他具有价值的应用场景。a)作为盲人或是有视觉障碍问题的病患的辅助系统，他们可以通过自然语言询问，就能获得细粒度的图像或者视频信息，能极大地帮助其获得场景的语义理解，在互联网和现实场景中均能作为一种便利的“视觉补充”。b)作为一种扩充人机交互的方式，在人机交互上可以实现多种的便利查询。通过对已有图像的询问，获得更深层次的背景知识，例如，对一副未曾见过的艺术名画询问其作者和作画背景，可以更深入的理解图像背后隐藏的人文和历史知识。通过源图像可以搜索具有相似“特征”的图像，例如，向系统查询一张埃菲尔铁塔的夜景图，将能获得更多具有相关特征的图像素材。同样可以通过图像描述查询到对应或者相似的图像。

\section{视觉问答的国内外研究历史与现状}
视觉问答任务广阔的应用场景和对人工智能发展的深远意义驱动着研究者不断细化、泛化视觉问答的问题深度、数据集构建、算法演化。

视觉问答的问题类型包含二值的是否问题\citing{krishna2017visual,zhu2016visual7w,andreas2015deep}、多选问题\citing{antol2015vqa,zhu2016visual7w}、开放性问题\citing{antol2015vqa}，涉及诸多计算机视觉中的子任务\citing{kafle2017visual}，例如：
\begin{description}[labelindent=2em, leftmargin=6em, style=sameline]
\item [物体识别]——图片中有哪些动物？
\item [物体检测]——图片中是否有小狗？
\item [属性分类]——图片中小狗的眼睛是什么颜色？
\item [场景分类]——图片中的小狗在什么地方？
\item [计数问题]——图片中有几只小狗？
\end{description}
除了以上列出的集中问题类型以外，在真实的人类交流中，更多的是具有更深层次、更复杂的问题。例如：“图片中有什么东西在伞下？”——需要能准确识别物体的空间位置关系、“图片中的交通路口是否可以通行？”——需要基于常识的推理、“图片中的汽车属于什么品牌？”——需要基于外部专业知识库提供隐藏信息。

对问题中涉及的计算机视觉任务细分，我们可以将视觉问答分为识别和推理两大任务范畴。就识别任务而言，包括物体识别、物体检测、属性分类、计数问题、空间关系判定，此类任务在以往的计算机视觉的研究中已经达到了较高的识别准确率，在某些物体识别和物体检测任务上已经能迫近甚至超越人类水平，虽然识别任务中仍然有许多值得研究的部分，但从研究的结构和难度上来看，只能算是相对简单的“点问题”。与之对应的便是更为复杂的”逻辑推演问题“，包括场景分类、知识库推理，常识推理可以被视为一种“隐知识库推理”。识别任务是逻辑推演的“前奏”，准确的识别将构成逻辑推演的“节点”，而逻辑链条中节点之间的“有向线段”则代表推理的过程(如图\ref{logic})，推理过程的构建才是推理任务的复杂之处，也是视觉问答任务的“最璀璨的明珠”。
\begin{figure}[H]
	\includegraphics[width=0.8\textwidth]{logic.png}
	\caption{逻辑链中的“节点”是文本或图像上的关键信息，例如物体名称、类别、属性、关系等，“有向线段”表示“节点”间的逻辑关系}
	\label{logic}
\end{figure}

受神经网络在计算机视觉和自然语言处理成功应用的影响，从2014至今的视觉问答研究多采用了神经网络模型，使用卷积神经网络CNN提取图像特征，使用卷积神经网络RNN或者长短期记忆LSTM处理文本信息，再通过不同的方式“融合”图像特征和文本特征得出答案。这种架构在识别任务相关的问题上能有可接受的表现，但在复杂推理任务上就显得性能堪忧，如果问题中涉及图像以外的物体时，系统很难得到令人满意的答案，这是因为训练集的不完备导致的。因此在复杂推理任务的解决上，一方面可以通过不断完善数据集的方式，让系统在训练过程得到足够多的“经验”，另一方面可以为系统提供额外的知识库，让系统能在其知识网络搜索、游走，通过逻辑链条得到正确且有意义的答案。

本小节将简要介绍视觉问答几个重要的数据集、常见的视觉问答算法以及几个知名的外源知识库。

\subsection{数据集}
从LeCun的MNIST数据集\citing{lecun1998mnist}到如今各式各样的计算机视觉等人工智能任务的数据集，优质的数据集已经成为了智能系统成长的“食粮”，尤其是神经网络复兴以来，计算机视觉和自然语言处理等任务的快速迭代蜕变一直都离不开数据的收集和整理工作\citing{deng2009imagenet,lin2014microsoft,rajpurkar2016squad}。关于数据更重要还是算法更重要的争论还继续，但数据集对于智能任务的训练价值是有目共睹的，当然，前提是数据集具有足够大的容量\citing{sun2017revisiting}、规范友好数据格式、较小的数据偏见等特点。视觉问答任务是在经历了计算机视觉和自然语言处理任务成功之后的新的颇具野心的构想——让系统能同时理解多模信息，并完成信息整合与推理。自从2014年以来，多个高质量的数据集为这个人工智能领域“新生儿”的快速成长提供了坚实的保障——DAQUAR\citing{malinowski2014multi}、COCO-QA\citing{ren2015exploring}、VQA\citing{antol2015vqa}、VQA 2.0\citing{goyal2017making}、CLEVR\citing{johnson2017clevr}、VQA-CP\citing{agrawal2018don}、KB-VQA\citing{wang2015explicit}。

\textbf{DAQUAR}
DAQUAR从NYU-Depth V2中带有语义分割标注的图片基础上扩展而来\citing{malinowski2014multi}。数据集包含1449张图片,图片多为室内场景，这大大地限制了数据集的场景丰富性，是该数据集的一大劣势。数据集由训练集和测试集两部分组成，训练集中包含6794对“问题-答案”，测试集中包含5674对“问题-答案”，“问题-答案”对由算法生成或是人类志愿者提供，算法生成的“问题-答案”对根据给定的模板生成，详见图\ref{DAQUAR}。

\begin{figure}[H]
	\includegraphics[width=0.8\textwidth]{DAQUAR.png}
	\caption{DAQUAR问题模板}
	\label{DAQUAR}
\end{figure}
DAQUAR数据集较小并且问题的类型只有三种：物体识别、色彩识别、计数，并且答案类型多以单词为主，因训练和测试系统复杂问题的推理能力较弱，偏于传统的物体识别任务。作为较早提出的针对视觉问答的数据集，为此后的数据集丰富工作提供了有益的方向。

\textbf{COCO-QA}
COCO-QA包含来自MS COCO的123287张真实场景图片，”问题-答案“对则是运用算法从MS COCO数据集的图片说明中生成的，为了方便生成算法的运用，将问题划分在物体识别、色彩识别、计数、地点查询四种类型。DAQUAR数据集在实际测试过程中，被发现仅仅通过简单的猜测答案的方式都能获得较高的正确率，这使得高准确率出现了极大的偏差，不能公正的测试系统的“推理”能力。为了克服该缺点，COCO-QA去除了出现频数极低和极高的一些答案，使得常见答案出现的评率从24.98\%下降到7.30\%。COCO-QA的训练集包含78736对“问题-答案”，测试集包含38948对“问题-答案”，在四个类别中的分布如图\ref{coco-vqa}。

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{coco-vqa.png}
	\caption{coco-vqa中“问题-答案”对的分布情况}
	\label{coco-vqa}
\end{figure}

\textbf{VQA}
VQA数据集是视觉问答领域发展的一个重要拐点，在此之前的数据集的问题类型被限制在一些模板之中，这使得数据集不能很好地测试出视觉问答系统在真实语境下的表现，例如，DAQUAR将答案仅仅限制在16种基本颜色和894种物体类别中\citing{malinowski2014multi}。VQA数据集中的问题和答案是无限制、开放式的，且全部由人类产生，同时图片的数量相较DAQUAR提高了两个数量级，到达254731张，极大的提高了数据集的容量。VQA数据集不仅包含从MS COCO\citing{lin2014microsoft}中提取的204721张真实场景的图片，还提供了50000张合成的抽象场景图（如图\ref{vqa-exmaple}），丰富了数据库场景的多样性，同时为高阶的场景推理和复杂空间推理提供了便利。

\begin{figure}[H]
	\centering
	\subfigure[真实场景图像实例]{
		\label{vqa-real}
		\includegraphics[width=0.4\textwidth]{VQA-real.png}}
	\subfigure[抽象场景图像实例]{
		\label{vqa-abs}
		\includegraphics[width=0.4\textwidth]{VQA-abs.png}}
	\caption{VQA中的真实、抽象场景图像实例}
	\label{vqa-exmaple}
\end{figure}
为了实现对复杂推理的训练和测试，VQA数据集在问题设置上采用了人工的方式，每张图片都有3个人类提出的问题。答案则分为开放式和多项选择两种形式，开放式答案由于答案并不唯一，因此难以确定标准答案，因此正确答案的评估方法也引入人工评估机制：对于同一个开放性问题由十个人分别作答，如果有三个及以上的被测者均提供了同一答案，该答案被视为正确答案，这意味着同一个问题，可能会出现几个正确答案，这符合人类世界的真实状况，答案的不唯一性为训练智能系统多角度、多层次的“认知”提供了可能。多项选择的答案则由四种类型、18个候选项组成（如图\ref{vqa-multi}）：
\begin{description}[labelindent=2em, leftmargin=6em, style=sameline]  
\item [正确答案] 一个，从被测者回答中取最为常见的作为正确答案
\item [混淆答案] 三个，不看图，仅根据问题作答的答案
\item [常见答案] 十个，数据集中最出现频数最高的十个答案
\item [随机答案] 四个，除去已经列出的选项，随机挑选四个答案
\end{description}
\begin{figure}[H]
	\centering
	\subfigure[真实场景图像的多选实例]{
		\includegraphics[width=0.8\textwidth]{vqa-multi.png}}
	\subfigure[抽象场景图像的多选实例]{
		\includegraphics[width=0.8\textwidth]{vqa-multi2.png}}
	\caption{VQA中的真实和抽象场景图像的多项选择实例}
	\label{vqa-multi}
\end{figure}

\textbf{VQA 2.0}
VQA数据集由于其建立了开放性问题和多项选择问题的评测标准受到了许多研究者的追捧，成为众多算法的测试数据集，但VQA数据集中的语言偏见问题也受到了研究人员的察觉和诟病。正是由于语言偏见的问题，即使是完全无视图像的算法也能在VQA数据集上得到49.6\%的准确率\citing{ren2015exploring}，这意味在VQA数据集的测试环境下，系统对于视觉信息的需求程度远远小于语言信息，这种状况相较于人类对于图像问答任务中的真实体验而言，是严重不符的。例如，答案为”是或否“的问题占所有问题的38\%，并且大约59\%的二值问题答案都为”是“；询问”什么运动“的问题中有41\%的答案为”网球；询问数量的问题中有39\%的答案为“2”。

针对以上问题，VQA 2.0应运而生。VQA 2.0通过在原有的VQA数据集基础上补充新的“混淆数据”实现数据集对视觉信息的增强。“混淆数据”和原始数据一样由（图像I，问题Q，答案A）的形式组织，不同的是新补充的图像与原有图像相似，但回答同样的问题Q却得到不同的答案A(如图\ref{vqa2})。针对同样的问题，在不同图片背景下需要得到不同的答案，这要求系统不仅能理解自然语言问题，同样需要关注图片的语义差异，才能得到正确的答案，这种平衡的方法能够筛选掉弱化图像理解的算法，强化了图像理解在视觉问答任务的重要性。补充后的VQA 2.0包含110万对”图像-问题“、20万张关联1300百万个问题的真实场景图片，数据量几乎是VQA数据集的两倍，势必会取代VQA数据集成为开放性问题的新测试标准。
\begin{figure}[H]
	\centering
	\subfigure[]{
		\includegraphics[width=0.8\textwidth]{vqa2-1.png}}
	\subfigure[]{
		\includegraphics[width=0.8\textwidth]{vqa2-2.png}}
	\caption{VQA 2.0中针对同一问题的不同图像和答案实例}
	\label{vqa2}
\end{figure}

\textbf{CLEVR}
为了更加准确地衡量视觉问答系统各个方面的推理能力，Johnson等人提出了一个结合语言和基本视觉推理诊断数据集CLEVR。CLEVR包含10万张由空间立方体组成的合成图像、将近100万个问题，其中包含85.3万个独特的问题。在图像的设置上，CLEVR为了减小识别难度，关注系统的视觉推理能力，采用了由空间立方体组成的合成图像，并且每张图像均有包含所有物体位置和属性的说明(如图\ref{clevr})。CLEVR的问题也均由程序生成得到，涉及属性识别、计数、比较、逻辑运算等子任务。

为了减少问题的偏见，数据集生成的问题中有85\%是独特的；为了控制问题的准确性，数据集剔除了有歧义的问题，例如，询问“正方体右边的球体是什么颜色？”时，如果“正方体”右边有多个“球体”，问题便产生歧义，答案变得不唯一，使得评估过程变得复杂和不准确；为了保持问题的复杂性，数据集拒绝了一些看似复杂但实际上限定条件无效的问题，例如，询问“球体前面的圆柱体是否为金属的？”时，如果场景中仅有一个“圆柱体”，那么问题中的“球体前面”的限定便可以被忽略，这种情况降低了问题的复杂性。

由于CLEVR数据集对图像和问题具有完全的掌控，能实现其他数据集难以实现的能力测试，要求系统具有短期的记忆力、注意力机制、组合推理能力。
但同样因为其简单的图像场景设置，CLEVR不能测试出视觉问答系统在常识推理、复杂推理的表现，并且也不能衡量系统在真实场景中的识别能力和稳定性。
\begin{figure}[H]
	\centering
	\subfigure[尺寸、形状、样色、材质的标注]{
		\includegraphics[width=0.3\textwidth]{clevr1.png}}
	\subfigure[空间左右的标注]{
		\includegraphics[width=0.3\textwidth]{clevr2.png}}
	\subfigure[空间前后的标注]{
		\includegraphics[width=0.3\textwidth]{clevr3.png}}
	\caption{CLEVR中图像标注}
	\label{clevr}
\end{figure}

\textbf{VQA-CP}
数据集一般会划分为训练集和测试集两个部分，在视觉问答任务中同样如此。如果训练集中的答案分布存在偏见，且测试集与训练集之间的答案分布相近，那么系统便可以通过记忆在训练过程中得到的数据偏见，并将其应用于测试过程，这样得到的准确率的可信度将会打折扣，例如，在训练集中询问颜色的问题中“白色”最为常见，同样在测试集中的“白色”也为热门答案，这回干扰系统的评估结果，不清楚系统是通过正确推理得到还是”经验“得到的。

针对训练集和测试集中问题答案分布相似的状况，VQA-CP重新对VQA数据集和VQA 2.0进行划分，重新划分得到的训练集和测试集在每个问题类型中的答案分布均不同，例如，在训练集中询问颜色的问题中，“白色”、“红色”为最常见的答案，而在测试集中“黑色”、“粉色”为最常见的答案；对于询问运动类型的问题，在训练集中“网球”为最多的答案，而在测试集中“滑雪”为最常见的答案(如图\ref{vqa-cp}）。
\begin{figure}[H]
	\centering
	\includegraphics[width=5in]{vqa-cp.png}
	\caption{vqa-cp训练集和测试集的答案分布}
	\label{vqa-cp}
\end{figure}

\textbf{KB-VQA}
回答VQA等数据集中的开放性问题可能涉及常识或者特定领域知识的先验知识，但已有数据集中还掺杂着大量不需要先验知识的训练样本，因此为了更好的评估VQA算法对需要高层次知识问题的准确推理能力，Wang等人构建了只包含复杂推理问题的数据集KB-VQA\citing{wang2015explicit}。

KB-VQA数据集从MS COCO\citing{lin2014microsoft}中挑选出700张图片样本，挑选出的图片包含150个物体类别和100个场景类别。每张图片附带有3-5个由人工生成的“问题-答案”对，所有的问题被限定在23种问题模板中，例如，“图片中是否存在某种概念？”，“图片中的某个物体被生产于什么地方？”等，详见\ref{qt}。

为了准确评估系统在需要先验知识的问题的表现，KB-VQA人工地赋予每个问题一个表示所需不同知识类型的标签，“视觉问题”、“常识问题”和“知识库问题”，其中“视觉问题”表示仅仅从图片中便可以获得答案的问题，例如，“物体是否存在于图片？”、“列出图片中包含的所有事物？”等，“常识问题”需要结合成人级别的常识和图像内容得出答案，例如，“图片涉及什么场景？”，“知识库问题”则需要某个领域特定的知识才能完成作答，例如，“图中的物品在哪一年被发明？”。23种问题模板在不同问题标签的分布如图\ref{qtd}。
\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{qt.png}
	\caption{KB-VQA中23中问题模板及对应的问题数量}
	\label{qt}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{qtd.png}
	\caption{KB-VQA中23中问题模板对应“视觉问题”、“常识问题”和“知识库问题”的分布情况}
	\label{qtd}
\end{figure}

数据集中的“视觉问题”、“常识问题”和“知识库问题”数量分别是1256、883和263，就图片和问题的数量而言，KB-VQA数据集相较于COCO-QA等数据集是非常小的，而且从图\ref{qt}也容易看出，有16种问题类型的问题数量都不超过100个，甚至有个位数的问题数量，数据集的不均衡和小容量很难准确得评估出系统在细分问题类型上的推理能力。但需要先验知识的问题占比要远高于大型数据集，DAQUAR\citing{malinowski2014multi}几乎全是“视觉问题”，COCO-VQA\citing{ren2015exploring}仅仅包含5.5\%的问题需要常识，没有问题需要额外的知识库。

KB-VQA在评估系统的复杂推理能力方面提供了一个解决方案，但数据集的容量、平衡性和多样性方面还需要更多的丰富，并且随着数据集的扩充，自动化和标准化的评估方式也相应的需要完善。

\textbf{FVQA}
为了评估视觉问答系统在需要先验知识的问题上的表现，Wang等人提出了FVQA数据集\citing{wang2017fvqa}。回答FVQA中的问题需要额外的知识，但不同于一般的数据集，FVQA将（图片，问题，答案）的三元组数据扩展为（图片，问题，答案，支持事实）的四元组形式，其中“支持事实”是回答问题所需要的额外知识，使用资源描述框架（RDF）的三元组形式，例如（猫，可以，爬树）。

FVQA从MS COCO\citing{lin2014microsoft}和ImageNet\citing{deng2009imagenet}中挑选出1906张图片，并对图片预处理，提取出三种类型的视觉概念：物体对象、场景和行为，最终提取出326种物体对象、21种场景和24种行为。为了获取与视觉概念相关的知识，FVQA以DBpedia\citing{auer2007dbpedia}、ConceptNet\citing{liu2004conceptnet}和WebChild\citing{tandon2014webchild}为知识源，从三种知识库中与视觉概念相关的所有知识中筛选出包含12种常见的谓语的知识，例如，关于分类的知识——“目录属于”、关于地点的知识——“地点所在”、关于大小比较的知识——“体积大于”，详见图\ref{fvqa_vc}。提取的知识以资源描述框架（RDF）的形式存储作为“支持事实”。
\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{fvqa_vc.png}
	\caption{从三种知识库中提取的知识涉及的12种谓语及相应的数量}
	\label{fvqa_vc}
\end{figure}

FVQA的问题和答案均使用人工的方式收集得到，被试者先选择图片中的一个视觉概念和一个与视觉概念相关的支持事实，再根据视觉概念和支持事实给出问题和答案，答案的来源要么是图片中的视觉概念要么是支持事实中涉及的概念。数据集最终包含4608个需要先验知识的问题，涉及3458条事实。根据视觉概念的类型，这些问题可以归为物体对象、场景和行为三种类型；根据支持事实的来源，可以归为DBpedia、ConceptNet和WebChild三种类型；根据答案来源，可以归为图片来源和知识库来源两种类型，不同分类在训练集和测试集的数量分布如图\ref{fvqa_cd}。
\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{fvqa_cd.png}
	\caption{不同分类在训练集和测试集的数量分布}
	\label{fvqa_cd}
\end{figure}

从统计的数据上不难看出，绝大多数问题是针对图像中的物体对象，这与提供的视觉概念中物体对象的高占比有强关联，从知识来源上分析，答案除了能从图像中获得外，还包含14\%的答案需要从额外知识库中获得，并且问题中不包含“是或否”的二值问题，这降低了系统”猜中正确答案“的情况。

FVQA和同样包含先验知识的数据集KB-VQA两者都能通过查询语言获取知识库中的数据，但不同于KB-VQA，FVQA拥更多的图片和问题数量，并且所有问题都需要额外知识。FVQA增加了ConceptNet和WebChild作为知识源，提高了知识库的多样性，能回答更多类型的问题，而不用预先设定问题模板。但FVQA数据集中几乎所有的答案都是物体对象，且为单个词语，不能训练模型给出对象关系的答案。FVQA数据集的支持事实多为单一谓语的句子，句式结构简单，如果用做训练集，不能考察模型应对多动词结构问题时的答案正确率。

然而两个数据集都面临着同样的问题：数据量的扩充和问题类型的扩充。两个数据集的问题收集都是通过人工的方式，并且参与者数量有限，因此直接导致了问题数量远低于其他自动化方法生成的数据集。大规模的协同工作和探索更多自动化方法是扩充数据集容量的方向。两个数据集都受到问题类型的限制，KB-VQA使用预先设定的问题模板，限制了问题的开放程度，FVQA虽然没有使用预先设定的问题模板，但其筛选的12种谓语间接的限制了问题的类型。扩展问题类型意味着额外知识库的扩充，也对视觉问答系统的问题解析提出了更高的要求，但这也能大幅的提高系统面对复杂多样的问题的开放性和鲁棒性。

\subsection{主流视觉问答算法}

视觉问答任务要求系统能同时正确理解问题文本内容和图像内容，一般而言视觉问答系统分为三个主要模块，a)从问题文本中提取特征，使得特征中包含足够多的语义信息。b)从图像中提取特征，理解图像中的物体信息、场景信息、活动信息、空间构成信息、颜色信息，将像素信息转化为系统可计算的数值量或者标签。 c)采用某种方式整合文本特征和图像特征，为系统建立一条高泛化能力、高稳健性、高准确率的答案生成通路。简而言之，视觉问答系统会图形和问题文本中分别提取特征，再将两者融合，最终以置信度高的候选答案作为输出（如图\ref{answer-generation}）。

从视觉问答的处理过程可以看出，算法的核心有三个部分组成：如何提取出高层次的图像特征，例如，物体、属性、场景等；如何挖掘问题文本中的语义信息，以求能深入的理解问题内容，确定答案的形式和内容；如何结合图像特征和文本特征，得出正确或是最佳答案。图像特征提取的方法都来自于计算机视觉的已有成果，一般使用预处理后的卷积神经网络，例如VGGNet\citing{simonyan2014very}、 ResNet\citing{he2016deep}和GoogLeNet\citing{Szegedy_2015_CVPR}。问题文本的特征提取则借鉴了自然语言处理中的成果，例如词袋模型（BOW）\citing{zhou2015simple}、长短期记忆（LSTM）\citing{malinowski2015ask}、门控复发单位（GRU）\citing{noh2016image,kumar2016ask,xiong2016dynamic}。系统输出答案的方式有两种(如图\ref{answer-generation})，最常见的方式是将任务视为分类问题，根据候选项的概率大小，确定答案。第二种方式则直接由系统遣词造句合成答案语句，此类方法多出现在有额外知识库的视觉问答系统中，例如Attributes-LSTM\citing{wu2016value}、ACK\citing{wu2016ask}、Ahab\citing{wang2015explicit}、Facts-VQA\citing{wang2017fvqa}、Multimodal KB\citing{zhu2015building}。
\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{answer-generation.png}
	\caption{常见的VQA方法是将图像和问题文本映射到同一特征空间，再组合融合两者的形成新的特征向量，特征向量作为分类器或者循环神经网络RNN（也可能是长短期记忆LSTM）的输入，输出得到最终的答案}
	\label{answer-generation}
\end{figure}

本节将简要介绍视觉问答方法中的联合嵌入模型、注意力机制以及动态记忆网络以及基于知识库的视觉问答方法。

\subsubsection{联合嵌入模型}
联合嵌入模型先将视觉信息和问题文本信息分别特征化，再通过特征向量串联\citing{zhou2015simple}、卷积\citing{ma2016learning}、逐元素相乘\citing{antol2015vqa}、逐元素相加\citing{malinowski2015ask}等池化方法融合图像特征和文本特征，最终得到最优答案。自从深度神经网络在计算机视觉和自然语言处理上的广泛应用以来，将各种模态的信息映射成特征向量的思想便大行其道，因此作为交叉领域中的视觉问答任务自然将多模信息联合嵌入到特征空间视为最“本能”的探索路径。

Malinowski等人首次提出了应用于真实场景视觉问答任务的联合嵌入模型Neural-Image-QA
\citing{malinowski2015ask}。Neural-Image-QA是一个由卷积神经网络CNN和长短期记忆LSTM组成的深度网络，先使用在ImageNet预处理过的卷积神经网络CNN对图像进行特征提取，得到的特征向量和问题文本一起传输到长短期记忆LSTM中，从而生成答案的单词序列。模型在DAQUAR数据集上完成训练和测试，对于答案只有一个词语的问题，准确率为19.43\%，对于答案是多个词语的问题，准确率为17.49\%。

Gao等人提出了mQA模型用于解决视觉问答任务\citing{NIPS2015_5641}，mQA由四个部分构成，用于提取问题特征的长短期记忆LSTM（Q）、用于提取视觉特征的卷积神经网络CNN、用于存储具有多词的答案的语义上文的长短期记忆LSTM（A）、用于融合问题特征和已有的部分答案的语义特征并且预测答案的下一个词语的部分。提取视觉特征的CNN采用在ImageNet分类任务上预处理的卷积神经网络，在训练过程中保持不变，训练其他三个部分，以达到最高的准确率。区别于Malinowski的Neural-Image-QA，mQA认为问题和答案在句法结构上有所不同，因此编码问题的LSTM和解码答案的LSTM为采用两个独立的网络，使用不用的权重矩阵,为了降低系统过拟合的风险，共享了词嵌入层。整体架构如图\ref{mQA}。
\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{mQA.png}
	\caption{mQA采用两个独立的LSTM编码问题序列和解码答案序列}
	\label{mQA}
\end{figure}

Noh等人认为单单使用相同权重参数的深度卷积神经网络去处理不同的问题，并期待能得到足够准确的答案，这是很困难的\citing{noh2016image}。因此他们提出DPPnet，在卷积神经网络CNN中添加一个动态参数层，动态参数层中的参数会根据问题的不同而改变，这使得每个问题输入都对应一个独特的分类网络。模型由三个部分组成，一个部分作为分类网络的卷积神经网络，第二个部分是参数预测网络，由门控复发单位编码问题序列，再通过一个全连接层输入动态参数，第三个部分是一个哈希函数，将参数预测网络输出的动态参数配置到分类网络中。如图\ref{DP-CNN}。
\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{DP-CNN.png}
	\caption{带有动态参数层的卷积网络模型DPPnet}
	\label{DP-CNN}
\end{figure}

Zhou等人同样适用预处理后的卷积神经网络CNN，但在处理问题文本时选择了比长短期记忆LSTM更为简单的词袋模型BOW，提出了iBOWIMG模型\citing{zhou2015simple}。iBOWIMG模型受到BOWIMG\citing{antol2015vqa}在VQA数据集上优于部分基于长短期记忆LSTM模型的启发，在原有基础上将VGGNet替换为在图像特征提取表现更优的GoogLeNet\citing{Szegedy_2015_CVPR}，将图像特征向量和文本特征向量串联后送入softmax层预测问题答案（如图\ref{iBOWIMG}），在COCO-VQA数据集上的测试展现出具有竞争力的表现。
\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{iBOWIMG.png}
	\caption{iBOWIMG使用词袋BOW模型作为词特征向量编码器}
	\label{iBOWIMG}
\end{figure}

Lin等人将卷积神经网络CNN不仅应用于编码图像内容，而且也应用于问题文本的提取\citing{ma2016learning}。在处理图像特征和文本特征时使用一个多模态的卷积层输出联合特征向量，再使用softmax层预测最终的答案。如图\ref{lin}。
\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{lin.png}
	\caption{图像特征和问题文本特征提取时均使用CNN}
	\label{lin}
\end{figure}

除了使用不同的方法提取图像和文本特征以外，联合嵌入模型的另一个能够显著改善模型准确率的方向就是实验不用特征向量融合的池化方法。Malinowski等人通过对不同的特征向量融合方法的比较，可以看出系统的准确率与特征向量融合方法有关，不同方法之间准确率最多能相差9个百分点之多\citing{malinowski2015ask}。除了以上提到的iBOWIMG采用向量串联的方式,Lin使用向量卷积的方式外，Antol等人提出的模型使用逐元素相乘的方法融合两者\citing{antol2015vqa}，Saito等人认为不同的特征融合方法各有特点，会保留或损失不同的特征，为了充分利用不同方法所保留的特征，提出了一种融合逐元素相加和逐元素相乘相结合的模型DualNet。模型同样利用了使用不同卷积神经网络CNN提取的图像特征，例如在真实场景图像采用了VGG-19\citing{simonyan2014very}、ResNet-152和ResNet-101\citing{he2016deep}。DualNet对提取出的文本特征和图像特征分别使用逐元素相加和逐元素相乘的方法得到两个不同的联合向量，再将两个的联合向量串联得到最终的合成向量，如图\ref{DualNet}。
\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{DualNet.png}
	\caption{DualNet针对真实场景图像的模型架构}
	\label{DualNet}
\end{figure}

Fukui等人认为向量之间的外乘运算中，所有元素之间的互动更加活跃，应该能保留更加丰富的特征信息，因此提出一种更为复杂的多模态紧凑双线性池化方法（MCB）。一般的双线性模型会对两个向量的外乘结果线性化，外乘操作会得到异常高维的向量，例如外乘的两个向量维度均为2048、输出向量维度为3000时，那么训练参数的数量将达到125亿个之多，这会导致巨大的计算开销。而提出的多模态紧凑双线性方法能避免直接计算向量外乘，同时保留了大量特征，模型架构如图\ref{mcb}。
\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{mcb.png}
	\caption{使用多模态紧凑双线性池化融合图像和文本特征}
	\label{mcb}
\end{figure}

\subsubsection{注意力机制}
人类获取外部视觉信息时，会自动形成一种“像素不均衡”，在同一视野范围内的像素被视觉中枢神经系统根据“关注区域”的远近、相关性特征自动分配不同的分辨率，使得“关注区域”内的像素具有极高的分辨率，而其他的像素仅仅作为视觉信息输入，并不参与大脑的语义处理（如图所示\ref{human-virtual}）。因此视觉注意力机制帮助大脑过滤了低相关性的视觉信息，减少了待处理数据的体积，极大地提高了信息处理速率并松弛了大脑负载。
\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{human-virtual.png}
	\caption{人类视觉系统的“像素不均匀”现象}
	\label{human-virtual}
\end{figure}

近几年，受到人类视觉注意力机制的启发，在神经网络中引入注意力机制变得十分热门，在自然语言处理和计算机视觉领域的应用也极大得帮助了原有算法精度和计算效率的提升。Google Deepmind团队提出了一种带有注意力机制的循环神经网络(RNN)，并成功应用于图像分类任务，获得了优于以往卷积神经网络(CNN)的基线水平的分类精度\citing{mnih2014recurrent}。随后，带有注意力机制的循环神经网络便被广泛应用于自然语言处理和计算机视觉的多个子领域\citing{bahdanau2014neural, xu2015show, NIPS2015_5847}。Bahdanau等人将注意力机制引入神经机器翻译任务，仍然使用“编码-解码”的翻译模式，但一改以往将源语言文本映射为一个固定长度的向量的编码方式，而是将原语言文本编码为向量序列，解码时将翻译和位置对应因素联合学习，训练向量序列中各向量对翻译词组的不同权重，加和完成翻译结果的推断，得到了以往最优的结果\citing{bahdanau2014neural}。Xu等人受到注意力机制在机器翻译和物体识别任务成功应用的启发，将带有注意力机制的循环神经网络应用于自动生成图片说明，并且在Flickr9k, Flickr30k 和MS COCO 三个数据集上均获得了最优的结果\citing{xu2015show}。随后，更多注意力机制的变型或优化研究均在图片说明任务上展开\citing{ 7243334, wu2017global, li2017image, lu2017knowing}。

相较起图片说明任务，视觉问答任务除了要求系统能理解图片内容，生成语义和句式合理的自然语言文本以外，还需要联合学习问题文本和聚焦与问题相关的图像细节。这些任务特性决定了视觉问答任务可以利用已有较为先进的图片说明任务的框架，同时融合自然语言处理的最新成果。注意力机制在自然语言处理和计算机视觉上的成功应用便成为了视觉问答算法快速发展的基石。

Chen等人最先将注意力机制引入视觉问答任务，提出了基于注意力机制的可配置卷积神经网络（ABC-CNN）用于针对“图像问题对”生成对应的注意力映射，将问题的语义信息和图像区域建立映射，使得答案生成取决于被关注区域，减少无关区域的影响\citing{chen2015abc}（模型架构如图\ref{abc-cnn}）。在Toronto COCO-QA\citing{ren2015exploring}, DAQUAR\citing{ malinowski2014multi}, 和VQA\citing{antol2015vqa}三个数据集上的测试结果都提升了最优结果，证明了注意力机制在提高视觉问答任务上的有效性，同时注意力权重图能反应系统的推理过程，为参数的微调提供了依据。
\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{abc-cnn.png}
	\caption{ABC-CNN使用CNN提取图像特征，LSTM提取问题文本特征，黄色方框内为用于推测与问题相关的图像区域的注意力机制}
	\label{abc-cnn}
\end{figure}

Shih等人使用简单的word2vec方法编码“问题-答案”对，使用预处理后的卷积神经网络CNN对图片的不同区域编码，将编码后的文本特征向量和图片特征向量映射到同一特征空间，根据特征之间的点乘运算决定每个图像区域的权重，最后结合权重化以后的图像特征和文本特征得出答案。架构如图\ref{shih}。在辨别物体颜色的任务上得到了最优结果\citing{ shih2016look}。类似的工作还有Ilievski等人提出的“聚焦型动态注意力模型“\citing{ilievski2016focused}。
\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{shih.png}
	\caption{使用图片区域选择层实现注意力机制的架构}
	\label{shih}
\end{figure}

包括以上提到的在内，多数注意力机制对问题文本和图像区域特征进行一次运算，直接生成图像注意力权重图。针对这种情况，Yang等人提出堆栈式注意力网络——使用问题的语义表达对图像进行多次查询，不断缩小答案相关区域，实现更高的精度\citing{yang2016stacked}。注意力机制在视觉问答上的其他应用还有，同时使用对图像和问题使用注意力机制的联合注意力模型\citing{NIPS2016_6202}；不采用图像区域赋值方法，而是过滤掉不相关区域的“自适应硬性注意力网络”\citing{malinowski2018learning}。

对于神经网络训练这类参数密集和计算密集的框架，注意力机制能带来两个重要的改变。一方面，无论对于图像输入还是文本输入，原有的方法都选择将输入看做一个整体，因此映射后的向量需要包含完整的输入信息，对于包含词组过多文本或是场景过于复杂的图像，编码后的向量根本无法区分开输入的局部特征，这使得神经网络的可解释性大大降低。引入注意力机制后，编码方式改变，将输入视为为局部信息的综合，保留了文本中单词和图像中像素区域的信息，通过可视化处理，能清晰的看出神经网络的推理过程，增强了系统的可解释性，可以称之为一种“弱化黑盒的处理”。另一方面，注意力机制非常符合人类对于语言和视觉信息的处理方式，这背后的假设是：针对绝大多数任务，只需要从信息源的局部便能获得充分正确的答案。类似于人类，具有注意力机制的智能体应当能获得更高的执行的效率和更高的答案精度。

\subsubsection{动态记忆网络}
无论是在自然语言理解还是图像内容理解，人类在获取单词或者图像像素区域的语义时不会将其与语境割裂来看，通常上下文语境对于准确理解文本和图像信息是非常重要的，因为在语言和图像中存在大量具有歧义特性的内容，例如，在语言中一个单词具有不同的语义，也可能有不同的词性，只有在上下文的语境中才能确定词语的真正含义。记忆力与上下文语境相似，是神经网络在训练过程中存储的“经验”，这种“经验”有助于以后的训练，这种累积经验能创造更准确的答案，基于这样的假设，研究人员为从序列化的输入中获得更准确的输入，而引入了动态记忆网络\citing{jiang2015compositional,kumar2016ask,xiong2016dynamic}。

Jiang等人在常见的CNN解析图像、LSTM解析问题文本的架构上，新增一个成分记忆模块\citing{jiang2015compositional}，旨在融合每一次训练过程中的局部图像信息和文本信息，并提供给下一次训练使用，从而使网络存储了训练过程的“经验”，这与之后提出的动态记忆网络有同样的思想，模型训练流程如图\ref{c-memory}。
\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{c-memory.png}
	\caption{成分记忆模型的训练流程}
	\label{c-memory}
\end{figure}

Kumar等人为解决文本问答（Text-QA）任务而提出动态记忆网络（DMN）\citing{kumar2016ask}。动态记忆网络（DMN）是一个用于生成文本问题答案的神经网络框架，它由输入模块、问题模块、情节记忆模块和问题模块构成，输入模块用于编码文本输入；问题模块用于编码文本问题；情节记忆模块接受由输入和问题模块得到的分布式向量，再使用注意力机制选择部分接受到的向量，结合选择后的向量与以往存储的“记忆”生成新的“记忆”向量，并不断迭代；答案模块根据最终的记忆向量生成答案，模型架构如图\ref{dmn}。
\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{dmn.png}
	\caption{DMN基础架构}
	\label{dmn}
\end{figure}

动态记忆网络（DMN）在文本问答、语义分析、词性标注任务上取得了最优的结果，受到其在处理序列化的文本信息上的优异表现的启发，Xiong等人在原有网络的基础上改善了输入和记忆模块，除了能处理文本信息外，还能处理图像信息，提出应用到于视觉问答任务动态记忆网络+（DMN+）\citing{xiong2016dynamic}，如图\ref{v-dmn}。动态记忆网络+（DMN+）将原有的输入模块中处理文本编码的门控复发单元（GRU）更换为双向门控复发单元（bi-GRU）以得到文本或图像区域更完整的上下文信息;使用基于注意力机制的门控复发单元替换原有的软性注意力机制。更新后的动态记忆网络+（DMN+）在DAQUAR\citing{malinowski2014multi}和VQA数据集\citing{antol2015vqa}上的测试结果都得到了具有竞争力的表现。
\begin{figure}[H]
	\centering
	\subfigure[应用于文本问答的动态记忆网络（DMN）模型架构]{
		\includegraphics[width=0.4\textwidth]{t-dmn.png}}
	\subfigure[应用于视觉问答的动态记忆网络+（DMN+）模型架构]{
		\includegraphics[width=0.4\textwidth]{v-dmn.png}}
	\caption{DMN+与DMN架构对比}
	\label{v-dmn}
\end{figure}

\subsubsection{基于外源知识库的视觉问答算法}
视觉问答任务基于图像场景回答问题，图像理解、问题理解和答案生成是实现准确的视觉问答系统的算法核心。图像理解、问题理解和答案生成三者又可以根据人类思考逻辑将其划分为两个逻辑层次，问题理解成为逻辑基点，图像理解和答案生成都根据问题的不同而采用适当的算法策略——注意力机制便是一种借助问题理解而实现计算效率更高的图像解析方法，答案生成中关心的答案类型和答案词组长度也需要依照问题的不同而选择。因此问题的解析过程对于视觉问答算法的准确性和计算成本都有很大的影响。

正如绪论中提及的，问题可以分为识别和推理两个大类，推理任务中既要求系统能准确识别图像中的对象，往往也会涉及图像中无法获取的先验知识。先验知识包括众所周知但不会显性呈现的常识和面对特定领域需要具备的专业知识，例如，判断路口是否可以通行时，涉及基本交通规则的常识，判断艺术品的作者这类专业问题时，需要借助与该艺术品相关的知识储备。

先验知识对视觉问答系统提出了更高的要求，这也揭露了主流的联合嵌入模型的缺陷：第一点，联合嵌入模型的答案生成来源于训练集中的问题和答案文本，这意味着训练集中包含的知识和文本内容是整个视觉问答系统的所有知识来源，因此对于测试集中涉及的全新概念或答案，系统根本无法得出正确的答案。不断扩充包含更多先验知识的训练集是提高精度的方式之一，但对于整个世界蕴含的不可计量的知识而言，这种方式是不实际的。第二点，联合嵌入模型要求网络本身能存储学习到的知识，目前网络的容量相较于需要学习的知识是严重不足的。第三点，神经网络海量参数和复杂网络连接带来的黑盒特性依然存在。对于识别和分类等问题而言，可解释性与高精确度相比，显得不那么重要，但是对于需要明确推理过程的问答系统而言，黑盒的不可解释性会降低提问者对系统的可信度，毕竟没人会轻易相信一个无法解释的答案。

一种可行的解决方案是将推理过程和知识学习分离，保留系统原有的图像理解和问题文本理解模块，在答案生成模块中引入外源知识库。可扩展的外源知识库可以解决网络容量的限制问题；知识库中结构化的数据可以作为推理过程的起点，知识库中的数据关联能为推理提供路径，形成逻辑链条，提高系统的可解释性。本节将对已有的基于知识库的视觉问答方法进行详细的介绍，分析各种方法的优势和存在的问题。

\textbf{Ahab}

Wang等人提出的Ahab视觉问答系统利用DBpedia作为知识库，实现对需要先验知识的问题的推理应答，即使问题中涉及不包含于图像中的概念\citing{wang2015explicit}。Ahab的主要思路为三点，第一点，将图像中的概念链接到知识库中相同的概念，形成从图像到知识库的映射，第二点，将自然语言的文本问题处理为知识库查询语句，实现从自然语言的句法和语义结构变换到相应的查询语句结构，第三点，将知识库的查询结果转换为自然语言表达。利用以上三点，Ahab可以不通过数据集训练获取知识，而使用自然语言到知识库的两次转化完成问答任务。

具体来说，为了建立图像概念到知识库实体之间的映射，首先检测图像包含的概念，再将提取出的图像概念和知识库实体建立链接。Ahab从图像中提取物体对象、图像场景和图像属性三种视觉概念，对象提取使用在MS COCO\citing{lin2014microsoft}和ImageNet\citing{deng2009imagenet}上预训练后的Fast-RCNN\citing{ren2015faster}，能实现224种类型的对象识别；场景分类器使用预处理于MIT Places205\citing{zhou2014learning}数据集的VGG-16 CNN，理论上能实现对205种场景的识别，每张图片选取分数前三的场景标签；图像属性提取器使用预处理于ImageNet\citing{deng2009imagenet}和MS COCO\citing{lin2014microsoft}的VGG-16 CNN，每张图片选取分数前十的属性标签。所有提取出的图像信息都使用资源描述框架（RDF）的形式表示，例如，“图像中包含长颈鹿对象”被表示为（图像，包含，对象1），（对象1，名称，长颈鹿）。每个视觉概念则被直接链接到具有相同语义的知识库概念，如图所示\ref{linkingMathord}。
\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{linkingMathord.png}
	\caption{Ahab中链接图像信息和知识库实体的RDF图结构}
	\label{linkingMathord}
\end{figure}
所有资源描述框架（RDF）数据被存储在OpenLink Virtuoso中——一个能存储多种数据类型的数据库。

Ahab使用Quepy开源框架将自然语言问题转化为相应的知识库查询语句，但Quepy解析问题时，需要预先设定的正则表达式模板，因此Wang等人使用KB-VQA数据集作为实验数据集。结合KB-VQA中的23种问题模板和针对不同问题类型的谓语选择，Ahab能根据不同的问题产生相应的查询语句,并得到问题答案。

Ahab类似于专家系统，针对特定的问题设定了与之对应的知识库查询方法，应用于知识库的搜索路径可以为视为系统“逻辑推理”的过程，因此Ahab不仅输出最终的答案，而且也将答案推理的过程作为输出，实现了对系统推理的显性表达，问题处理的过程如图\ref{question_processing}。
\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{question_processing.png}
	\caption{Ahab结合问题文本和预先设定的模板，解析出问题中的概念，再将问题中的概念与知识库实体建立链接，并生成查询语句，得到查询结果和推理过程}
	\label{question_processing}
\end{figure}

在评估Ahab对于需要先验知识的问题的表现时，Wang等人使用了自己构建的KB-VQA数据集（上文中有详细介绍）作为测试集，但KB-VQA中的问题多数是开放性的，并且还没有自动化评估正确性的方法被提出，因此使用人工的方式对结果的正确性进行评估，每个结果被人工地赋予5种表示正确程度的分数：1分-完全错误、2分-部分错误、3分-模棱两可、4分-基本正确、5分-完全正确。

作为对比，评估还引入了由人类作答和主流的联合嵌入模型作答两种方式——使用CNN编码图像特征，LSTM编码问题文本和生成答案的模型，三种测试系统在不同问题的正确率和平均得分如图\ref{ahab_evaluation}。
\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{ahab_evaluation.png}
	\caption{Ahab、联合嵌入模型和人类作答在23种问题上的表现。Accuracy是得分超过3的问题数量的比例，Correctness是某类问题得分的加权平均数。}
	\label{ahab_evaluation}
\end{figure}
KB-VQA将23种问题类型划分为“视觉问题”、“常识问题”、“知识库问题”三个知识等级，如图\ref{qtd}，三种不同方法在三种知识等级的正确率统计如图\ref{type_evaluation}。
\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{type_evaluation.png}
	\caption{三种不同方法在三种知识等级的正确率统}
	\label{type_evaluation}
\end{figure}

从图\ref{qtd}中可以看出，LSTM的联合嵌入模型在“判断动物类别”、“判断对象生产年份”、“列出不同对象的共同属性”、“列出食物营养成分”、“判断最大/最小的物体”、“列出动物的近亲”这六种任务中正确率为0，其中除去“判断最大/最小的物体”为视觉问题外，其余5种问题均需要系统结合额外的知识回答，这正是基于训练集的概率模型的劣势——对于复杂关系和长知识链条的学习能力。总体上看，Ahab在每种问题类型上都优于联合嵌入模型，但离人类的正确率还是有一定差距，尤其在“判断物体颜色”和“比较两个物品的诞生先后”两种问题。

对于“列出与某个物品相同年份的物品”这类问题上，Ahab以75\%的准确率高出人类的50\%，但值得注意的是，KB-VQA数据集中此类问题只有4个，也就是说Ahab只是比人类多答对一个问题，考虑到答案生成过程和正确性评估过程中可能产生的误差，这并不能肯定的表明Ahab系统在此类问题上优于人类的表现。同样的状况也出现在其他问题类型上，因此KB-VQA在不同问题类型上数量的不均衡（问题数最多的类型与问题数最少的类型数量相差两个数量级）和问题样本数过小（16种问题类型的数量小于100，其中有两种的问题数量小于100）在评估视觉问答系统的真实推理能力上不能产生置信度足够高的结果，丰富数据集的样本和均衡不同类型的样本数量才能更好得评估系统的推理表现。

除了测试集存在稳定性较低和样本数较少的问题以外，Ahab系统只能针对预先设定的23种问题类型，这大大限制了问题的开放程度，不能满足真实的问答环境中海量的问题类型。而且Ahab的高正确率还建立在针对性地生成不同的问题查询语句之上，当问题类型数量剧增时，人工的对每种类型设定对应的算法是不切实际的，因此Ahab系统的扩展性也面临挑战。

但相较于主流使用统计方法的联合嵌入模型，Ahab利用知识库取代知识学习过程的方法在复杂推理任务，尤其是需要运用先验知识的问题上，实现了更好的系统表现，也为解决复杂推理问题的方法上提供了有益的实践。

\textbf{FVQA}
Ahab将问题解析为知识库查询语句时，需要预先确定问题模板，这极大的限制了系统面对多样化问题的能力，因此Wang等人改变了问题到查询语句的映射方式提出了FVQA模型\citing{wang2017fvqa}。FVQA模型使用带有FVQA数据集，FVQA数据集中的数据格式为（图片，问题，答案，支持事实），支持事实是一个包含答案的资源描述框架（RDF）数据，例如，（猫，能，爬树）。FVQA数据集中的问题包含三个属性：视觉概念（包含物体对象、场景和行为三种类型）、谓语（12种类型）和答案来源（图像和知识库两种）。FVQA模型在训练阶段，从标注后的支持事实中提取出问题的三种属性（VC表示视觉概念类型、REL表示谓语类型、AS表示知识来源类型），由于FVQA数据集中三种属性之间的组合能形成28类问题，所以FVQA模型使用长短期记忆（LSTM）网络训练一个28类的查询语句分类器，实现将问题到查询语句的分类过程，28种查询类型及其在训练/测试集的分布如图\ref{fvqa_query}。
\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{fvqa_query.png}
	\caption{FVQA模型的28种查询类型及其在训练/测试集的分布情况}
	\label{fvqa_query}
\end{figure}

通过LSTM的分类，文本问题被映射为（REL,VC,AS）的查询类型，对于所有28种查询类型，查询语句都由下面的形式构成：
\begin{verbatim}
Find ?X, ?Y, subject to 
	{(ImgID,Contain,?X) and (?X,VC-Type,VC) and (?X,REL,?Y)}
\end{verbatim}
其中ImgID表示图片的标号，?X表示在图片ImgID中类型为VC的视觉概念，?Y表示在知识库中与?X通过谓语REL链接的概念。再根据AS是图片还是知识库的类型，使用不同的方法得到最终的答案。

FVQA模型中最核心的部分是将问题映射为对应的查询类型，图\ref{qqmaping}展示了问题到查询映射模型（QQmaping）在测试集中对应的三个知识库的正确率，可以看到映射模型在WebChild上实现了超过90\%的高正确率，但在其他两个知识库的准确率就相对较低，分析其中原因，可能是训练集和测试集中问题表达形式相似度的影响。WebChild中的谓语由图\ref{fvqa_vc}可知，都是两者比较的词汇，因此问题的表达形式较为单一，例如，“在图中什么物体更为<形容词的比较级>？”。
\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{qqmaping.png}
	\caption{问题到查询映射模型（QQmaping）在测试集中对应的三个知识库的正确率/测试集的分布情况}
	\label{qqmaping}
\end{figure}
引入支持向量机（SVM）\citing{chang2011libsvm}和使用长短期记忆（LSTM）的联合嵌入模型\citing{wu2016value}作为基线模型：只提供问题的SVM-Question和LSTM-Question、只提供图片的SVM-Image和LSTM-Image以及同时提供问题文本和图片的SVM-Question+Image和LSTM-Question+Image，各模型使用FVQA数据集作为训练和测试集中测试，不同模型的正确率见表\ref{fvqa_table}。
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage[table,xcdraw]{xcolor}
% \usepackage{booktabs}
% If you use beamer only pass "xcolor=table" option, i.e. \documentclass[xcolor=table]{beamer}
\begin{table}[H]
% \resizebox{0.5\textwidth}{!}{}
\caption{不同模型在FVQA数据集上的测试正确率，Top-1表示只取得分最高的预测结果，Top-3和Top-10以此类推。灰色数据表示使用与问题对应的完全正确的查询类型时的正确率。}
\begin{tabular}{lccc}
\toprule
\multicolumn{1}{c}{} & \multicolumn{3}{c}{Overall Acc. (\%)}\\
\cmidrule(r){2-4}
\multicolumn{1}{c}{\multirow{-2}{*}{\textbf{Method}}} & \textbf{Top-1}& \textbf{Top-3} & \textbf{Top-10} \\
\midrule
SVM-Qusetion        & 11.19 & 20.68 & 32.14 \\
SVM-Image           & 17.55 & 30.75 & 49.02 \\
SVM-Qusetion+Image  & 17.99 & 31.83 & 49.55 \\
LSTM-Question       & 10.30 & 18.26 & 31.02 \\
LSTM-Image          & 22.69 & 36.21 & 58.59 \\
LSTM-Question+Image & 23.37 & 37.02 & 52.51 \\
\midrule
\cellcolor[HTML]{C0C0C0}gt-QQmaping & \cellcolor[HTML]{C0C0C0}64.23 & \cellcolor[HTML]{C0C0C0}71.58 & \cellcolor[HTML]{C0C0C0}72.74 \\
top-1-gt-QQmaping & 53.63 & 60.70 & 61.59 \\ 
top-3-gt-QQmaping & \textbf{58.19} & \textbf{65.89} & \textbf{66.83} \\
\bottomrule
\end{tabular}
\label{fvqa_table} 
\end{table}

从表\ref{fvqa_table}中Top-1一列可以看出，无论是SVM-Question+Image与SVM-Image之间的正确率差距还是LSTM-Question+Image与LSTM-Image的正确率差值都非常小，这说明问题的解析对于SVM和LSTM这两种模型正确率的提升没有太大的帮助，而两个模型总体的正确率也处于较低的水平，说明统计方法在样本较小的语料库中很难学习到知识间真正的逻辑关联。而FVQA模型使用问题到查询映射模型能从问题文本中提取到关键信息，并能利用关键信息组成有意义的语言结构，再结合额外知识库搜索到正确答案，答案获得的过程反映了推理的过程。gt-QQmaping（灰色背景）使用问题对应的正确查询类型，因此正确率反映了理想状况下FVQA模型从查询类型到生成查询语句过程中的误差情况，知识库查询过程的错误率在30\%左右。top-1-gt-QQmaping与gt-QQmaping之间的差距则代表问题到查询类型70.37\%（见图\ref{qqmaping}）的正确率在最终答案的影响，top-3-gt-QQmaping的准确率高于top-1-gt-QQmaping的原因在是因为前者拥有更高的问题到查询类型映射的准确率。

表\ref{fvqa_answerSource}提供了不同方法在不同答案来源上的正确率，对比表中Image和KB两列容易看出，答案来源于视觉概念的准确率在所有模型上均远高于知识库来源，这说明表中涉及的三种模型都只能从图像和问题文本中包含的概念中提取答案，一旦答案涉及都额外知识库中的“新”概念，准确率便急剧下降，即使是使用额外知识库的gt-QQmaping。
\begin{table}[H]
% \resizebox{0.8\textwidth}{!}{}
\centering
\caption{不同方法在不同答案来源上的正确率}
\begin{tabular}{lcccccc}
\toprule
\multicolumn{1}{c}{\multirow{3}{*}{\textbf{Method}}} & \multicolumn{6}{c}{Answer-Source}\\
\cmidrule(r){2-7}
 & \multicolumn{3}{c}{\textbf{Image}} & \multicolumn{3}{c}{\textbf{KB}}\\
\cmidrule(r){2-4}
\cmidrule(r){5-7}
 & \textbf{Top-1} & \textbf{Top-3} & \textbf{Top-10} & \textbf{Top-1} & \textbf{Top-3} & \textbf{Top-10} \\
 \midrule
SVM-Qusetion        & 12.80 & 24.53 & 36.48 & 0.68 & 2.03 & 3.72 \\
SVM-Image           & 19.92 & 34.88 & 55.11 & 2.03 & 3.72 & 9.12 \\
SVM-Qusetion+Image  & 20.43 & 36.07 & 55.73 & 2.03 & 4.05 & 9.12 \\
LSTM-Question       & 11.71 & 20.49 & 34.21 & 1.01 & 3.72 & 10.14 \\
LSTM-Image          & 25.49 & 40.40 & 65.12 & 4.39 & 8.78 & 15.88 \\
LSTM-Question+Image & 26.01 & 41.12 & 58.05 & 6.08 & 10.14 & 16.22 \\
\midrule
\cellcolor[HTML]{C0C0C0}gt-QQmaping & \cellcolor[HTML]{C0C0C0}72.65 & \cellcolor[HTML]{C0C0C0}80.13 & \cellcolor[HTML]{C0C0C0}80.13  & \cellcolor[HTML]{C0C0C0}9.12 & \cellcolor[HTML]{C0C0C0}15.54 & \cellcolor[HTML]{C0C0C0}24.32 \\
top-1-gt-QQmaping & 60.89 & 68.27 & 68.27 & 6.08 & 11.15 & 17.91 \\ 
top-3-gt-QQmaping & \textbf{66.10} & \textbf{74.15} & \textbf{74.15} & \textbf{6.42} & \textbf{11.82} & \textbf{18.92} \\
\bottomrule
\end{tabular}
\label{fvqa_answerSource}
\end{table}

FVQA模型提出了一种以句法结构中的谓语为核心的先验知识问题的解答思路，首先从问题中解析出关键的谓语信息，在问题到查询类型模型中，结合谓语、视觉概念和答案来源决定了28种不同的查询类型，再使用生成的查询语句搜索基于12种谓语构建的知识库，最终预测答案。“主语-谓语-宾语”的一般句式结构中谓语表示了主语和宾语之间的相互作用，即使在相同的主语和宾语情况下，不同的谓语能表达出截然不同的语义信息，而绝大多数问题也能够直接通过谓语，推断答案的范畴。以谓语为基础的优势有几点，第一，易于问题分类。问题的自然语言表达方式众多，但无论如何改变句式结构，表达相同含义的谓语有限，通过对谓语的语义划分能够划分出问题的不同类型。第二，便于知识库的查询。知识库中的实体之间通过不同的谓语连接，形成错综复杂的知识网络，一个实体有众多连接，但一个谓语只连接两个实体，且往往谓语的两端就是问题的答案。

FVQA模型的缺陷有三点，第一，分类数量的确定和分类模型的精度。FVQA模型由于查询语句的生成依赖于查询类型，因此问题到查询类型映射的准确性会直接影响到答案生成的正确率。表\ref{fvqa_table}是在FVQA数据集上进行的，由于数据集中问题的类型只有28种，因此FVQA模型在问题到查询类型映射模型中使用了28类的分类器，但在实际问答环境中问题类型的具体数量远远多于28种，且无法预先确定，想训练能应用于实际情景中的FVQA模型不仅需要数据集的扩充，还需要提高模型本身的分类精度。第二，不能回答以谓语为答案的问题。所有28种查询类型都要求能从问题中提取关键谓语，并且所有答案都是物体对象，如果问题询问对象之间的关系，模型则无法从问题文本中获得谓语，不能得到答案。第三，不能很好的处理含有多个动词的复杂推理问题。FVQA模型的查询语句过于简单，仅仅将一次查询结果作为答案，在面对需要多级推理的问题时，便无法直接得到答案，如表\ref{fvqa_wrong}。
\begin{table}[H]
% \resizebox{0.8\textwidth}{!}{}
\centering
\begin{tabular}{lcccccc}
\toprule
\multicolumn{2}{c}{\includegraphics{cow.png}} \\
\multicolumn{2}{c}{What is related to the animal that can be found in this place?} \\
\midrule
\multirow{2}{*}{Mined Facet:} & You are likely to find a cow in a picture \\
 & The cow is related to the tree \\
Predicted Answer: & cow \\
Ground Truth: & tree \\
\bottomrule
\end{tabular}
\caption{问题中包含related和found两个动词，FVQA模型只能提取一个作为谓语并查询，选择found为谓语时，会出现错误的预测}
\label{fvqa_wrong}
\end{table}

\textbf{基于知识库的通用嵌入模型}
无论是Ahab还是FVQA模型都是通过将问题固定在一定类型，再通过对应不同问题的类型的额外知识库查询，获得更丰富的知识，从而提高复杂问题的正确率。为了提高视觉问答系统的问题的灵活性，Wu等人又通过改进常见的CNN+LSTM的嵌入模型，提出了基于知识库的通用嵌入模型。模型的基本架构由图像属性提取网络（CNN）、图像描述生成网络、外部知识库查询网络以及答案生成网络（LSTM）构成，模型架构如图\ref{KBLSTM}。
\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{KBLSTM.png}
	\caption{结合外部知识库的通用嵌入模型}
	\label{KBLSTM}
\end{figure}

图像属性提取网络将图像属性提取问题视为多标签的分类问题，以图像的多个子区域作为输入，输出前五个从MS COCO中筛选得到的图像属性$V_{att}(I)$，属性可能为物体名称、动作或者描述特征的形容词。提取出的图像属性分别作为图像描述生成网络和外部知识库查询网络的输入，图像描述生成网络将\cite{wu2016value}中的高层次的属性表达输入LSTM网络生成基于图像属性的描述，再将文本描述转化为五个特征向量，平均池化所有向量得到向量$V_{cap}(I)$。外部知识库查询网络首先分别将五个图像属性转化为知识库查询语言，查询到DBpedia知识库中相应的对象后，返回其“comment”——“comment”往往包含关于知识库对象最重要的解释信息，如图\ref{SPAQLexample}，为了将大段的“comment”转化为向量，Wu等人使用Doc2Vec\citing{le2014distributed}——一种能方便的将句子、段落甚至文章等不固定长度的文本转化为固定大小的向量的模型，并且能包含文本内容的语义信息。——将其转化为$V_{know}(I)$。最后将$V_{att}(I)$、$V_{cap}(I)$、$V_{know}(I)$以及问题文本作为答案生成网络（LSTM）的输入，训练网络生成答案。
\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{SPAQLexample.png}
	\caption{使用‘dog’属性的查询语句以及返回的‘comment’内容}
	\label{SPAQLexample}
\end{figure}

在评估模型准确率时，由于该模型面对开放性问题，因此不同于Ahab和FVQA只能使用专门设计的数据集，该模型采用Toronto COCO-QA\citing{ren2015exploring}和VQA\citing{antol2015vqa}两个数据集进行评测。为了说明该模型的正确率情况，引入基线模型和几个结果较优的模型正确率作为对比，GUESS表示随机猜测模型，VggNet-LSTM使用在ImageNet上预处理的VggNet网络连接LSTM得到，VggNet+ft-LSTM使用在图像属性分类任务上微调后的VggNet，Att-LSTM表示直接将$V_{att}$作为LSTM输入的变体模型，类似的变体有Att+Cap-LSTM、Att+Know-LSTM、Att+Cap-LSTM、Cap+Know-LSTM，以及完整的Att+Cap+Know-LSTM模型。不同模型在两个数据集的正确率分别如表\ref{Wucoco}和\ref{Wuvqa}。
\begin{table}[H]
% \resizebox{0.8\textwidth}{!}{}
\centering
\caption{不同模型在Toronto COCO-QA正确率的表现}
\begin{tabular}{lc}
\toprule
\textbf{Toronto COCO-QA} & Acc(\%)\\
\midrule
GUESS\citing{ren2015image} & 6.65 \\
VggNet-LSTM & 50.73 \\
VggNet+ft-LSTM & 58.34 \\
\midrule
Att-LSTM & 61.38 \\
Att+Cap-LSTM & 69.02 \\
Att+Know-LSTM & 63.07 \\
Cap+Know-LSTM & 64.31 \\
\midrule
Att+Cap+Know-LS & \textbf{69.73} \\
\bottomrule
\end{tabular}
\label{Wucoco}
\end{table}

\begin{table}[H]
% \resizebox{0.8\textwidth}{!}{}
\centering
\caption{不同模型在VQA数据集的正确率表现}
\begin{tabular}{lc}
\toprule
\textbf{VQA} & Acc(\%)\\
\midrule
VggNet-LSTM & 44.93 \\
\midrule
Att-LSTM & 51.60 \\
Att+Cap-LSTM & 55.04 \\
Att+Know-LSTM & 53.79 \\
Cap+Know-LSTM & 52.31 \\
\midrule
Att+Cap+Know-LS & \textbf{55.96} \\
\bottomrule
\end{tabular}
\label{Wuvqa}
\end{table}

\subsection{常见知识库}

\subsubsection{知识库的研究历史}
人类智能体通过学习和实践不断获取知识与经验，并能将习得的知识存储在记忆系统中，面对相关问题时能准确、快速地调用相关的知识和经验，完成识别和推理过程，成功解决问题。人工智能系统的终极目标便是能像人类一般快速、准确地解决未知问题，甚至超越人类的物理极限，实现范围更广、更艰深的任务解决。人类在真实世界中的学习是不断将非结构化的信息重构为结构化的知识的过程，知识库（KB）是一种包含常识和描述真实世界的事实的知识集，在不同的应用情景中有不同的内部结构。

知识库最早被应用于人工智能中的专家系统\citing{akerkar2010knowledge}，专家系统是一种建立在知识库基础上，使用推理方法完成复杂推理过程，最终实现与人类专家同水平的决策能力的计算机系统，被广泛应用于医学诊断、分子结构推理、自然语言理解等领域。专家系统面向的专家任务需要特定领域的知识，这也使得知识库成为专家系统的核心之一。针对不同领域的任务构建知识的表达方式是困难的，因为专家知识可能是不精确的，同时要从知识库中获取答案的过程依赖于人工的制定复杂的规则，知识库精度和人力成本等因素制约了专家系统在更多领域的应用。

知识库也被应用于在自然语言处理的任务，例如机器翻译和文本问答。知识库中的本体包含某个领域中的各种概念和概念间的关系，本体在机器翻译中可作为知识源\citing{nirenburg1994machine}。语言学中的多义词在不同的语境中被解释为不同的含义，人类能根据上下文语境的不同选择出最恰当的词语，但对于机器翻译系统便是一大难题。当机器翻译系统能够获得足够多的本体作为知识源时，能较好地解决多义词的解释问题，从而得到更加准确的翻译结果\citing{knight1993building}。

文本问答系统在早期作为专家系统的交互界面，在之后的发展中逐渐独立出来成为自然语言处理的一个分支，文本问答系统根据给出的文本问题，从文本知识库中提取答案，此时的文本知识库往往是文本组成的文档，还未使用资源描述框架（RDF）的结构化数据。大多数文本问答系统都采用相对标准的结构：根据问题文本建立查询、利用信息提取方法（IR）确定可能包含答案的文章位置、进一步确定答案所在的片段，这种架构下不使用任何与答案相关的额外知识\citing{hermjakob2000knowledge}。Hermjakob等人提出了将手写规则和概念本体相结合的问答系统——Webclopedia\citing{hermjakob2000knowledge}。Webclopedia由对输入问题进行句法和语义的解析的问题解析模块、用于文档查询的查询模块、用于获得与答案相关的文档的信息提取模块、片段解析模块、答案匹配模块和答案生成模块构成。系统在多个模块中使用了知识库提高精确度，在问题解析过程中使用了语言知识库——由30000个节点的概念层级、140个问题/答案类型和词库组成，帮助系统确定问题的句法结构；在查询模块中使用了WordNet\citing{miller1995wordnet}扩展与问题关键词关联的信息；在答案匹配模块中也使用了常识和事实知识库，系统的架构如图\ref{Webclopedia}。
\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{Webclopedia.png}
	\caption{Webclopedia系统架构}
	\label{Webclopedia}
\end{figure}

应用信息提取技术（IR）的问答系统有一个非常明显的缺点——只能根据问题确定答案相关的文章或者段落，不能给出更为直接的答案。为解决这种缺陷，研究人员探索了更多的方法。

Burke等人一改通常的从文章中提取答案的方式，先将被频繁问到的问题（FAQ）以“问题-答案”对的形式存储为知识库，再从新问题中寻找与知识库匹配程度最高的“问题-答案”对，进而获得答案\citing{burke1997question}。在此方法中最核心的步骤是对新旧问题之间的匹配，为了使匹配的问题之间的语义相似度最大，系统还使用了WordNet\citing{miller1995wordnet}的语义知识，WordNet能提供词语和其同义词集合、同义词集合之间的关系，因此能避免一些匹配过程中的歧义错误，提高匹配的准确度。这里以“匹配”为核心思想的算法最大的障碍是常见问题集的容量、深度和广度问题，因此通常对于范围较小的场景而言，才能实现较好的匹配准确度。Rinaldi等人提出一个专门针对技术领域的基于知识的问答系统ExtrAns\citing{rinaldi2002towards}。ExtrAns以技术手册为知识库，将问题文本和知识库都转化为一种称为“最小逻辑形式”（MLF）的语义表达，并通过逻辑证明提取出答案。

随着资源描述框架（RDF）在构建知识库的兴起，知识库也由原来的文档形式转化为冗余更小、可扩展性更强、易用性更强的结构化数据库，这也促进了基于知识库的视觉问答方法的兴起。

\begin{comment}
\section{常见的知识库}
知识表达是人工智能中历史较为悠久的领域，并且大量的知识表达模型被提出，从最早的“框架和脚本”（frame and script）\citing{minsky1974framework,schank2013scripts}到后来的逻辑表达方式、资源描述框架（RDF）和网络本体语言（OWL）。随着互联网技术的普及，海量的网页、文章、超文本、图片等多种模态的资源被创造，如何将这些海量松散的多模数据重组为结构化的数据成为计算机科学的重要任务之一，大量的研究对信息的整合进行了探索\citing{smith1981multibase,wiederhold1993intelligent,subrahmanian1994amalgamating,embley1998ontology,alani2003automatic}，语义网和相关技术的出现促进了大尺度知识库的发展，出现了DBpedia\citing{auer2007dbpedia}、OpenIE\citing{banko2007open}、Yago\citing{suchanek2007yago}、Freebase\citing{bollacker2008freebase}、Wikidata\citing{vrandevcic2014wikidata}等多种含有常识和特定领域知识的知识库。

除了知识库以外还有一种常用的存储数据的方式——数据库。数据库通常被组织成表格的方式，表内使用数值或者字符的方式表示数据，首行罗列出不同的属性，其余行则代表存储的数据。数据库中的表与表之间存在的指针代表表之间的联系。不同于数据库，知识库不以表为组织形式，而是由大量形式为（主语，谓语，宾语）的三元组构成的图结构，这样的三元组以资源描述框架（RDF）为模型基础\citing{lassila1997resource}，能够方便的通过查询语句获得信息。“主语”和“宾语”表示知识库中的实体，“谓语”代表两者之间的关系。例如，将“猫种类属于哺乳类”转化的三元组形式为（猫，种类属于，哺乳类），“猫”是主语，“种类属于”是谓语，“哺乳类”是宾语。数据库与知识库的结构对比如图\ref{db-kb}。
\begin{figure}[H]
	\centering
	\subfigure[数据库的表结构]{
		\includegraphics[width=0.5\textwidth]{database.jpg}}
	\subfigure[知识库的由三元组构成的图结构]{
		\includegraphics[width=0.7\textwidth]{kb.png}}
	\caption{数据库与知识库不同的数据组织形式}
	\label{db-kb}
\end{figure}

知识库具有高通用性、高可读性的特征。通过资源描述框架（RDF）能将所有现实中可描述的事物和事物间的关系组织到知识库中，这种通用性特征能够极大地提高了自动化系统存储、交换和使用信息；三元组的结构来源于语言学中“主谓宾”的基本语句构成方式，这既符合人类的认知方式，也是一种简单的数据组织形式，因此高可读性即使针对人类可读也是针对机器可读。正是因为知识库是建立在真实世界事实的描述之上，因此它能成为复杂决策和推理的基石，从人类推理过程看，知识库便是复杂推理的“起点”。
\end{comment}

\textbf{Yago}
知识库通常由人工和自动化提取两种方式构建得到，对比这两种不同构建方式，自动化提取的知识库往往质量较低，容易包含错误信息，而人工构建的知识库能满足较高的精度要求，但由于人工构建的成本较高，因此此类知识库有数据容量受限、构建周期长、内容老化快等缺陷。

Suchanek等人结合Wikipedia文章的广博性和WordNet优秀的语义分类，提出了自动化生成本体的知识库YAGO\citing{suchanek2007yago}。Wikipedia的文章对某个话题或概念进行详细的多角度说明，同时大多数文章都归属于一个或者多个类别，类别页面既包含了大量实体和概念，可以作为知识库中的本体，同时类别页面也隐含着概念之间的平行关系和所属关系，这能提供一定的结构关系。YAGO利用Wikipedia目录页面提取出其中的实体和实体之间的关系，同时结合WordNet中概念的清晰层次关系，实现了97\%的准确率。初始版本中涉及90万个实体和500万个实体之间的关系。

YAGO被设计为可扩展的知识库，能够结合特定领域的知识源或是从网络上提取得到的信息构建领域相关的知识库，因此之后的研究者也在此基础上进行了多种的扩展。YAGO2在YAGO基础上引入GeoNames——包含超过700万个地点信息，在“实体-关联”的表示方法中加入了时间和空间维度，不仅能丰富事实的准确性，还能反应出实体在时空层面的变化\citing{hoffart2013yago2}。YAGO3构建了一个多语言的知识库\citing{mahdisoltani2013yago3}。

\textbf{DBpedia}
Wikipedia是由非盈利组织维基媒体基金会（Wikimedia Foundation）构建的世界上最大的多语言的开放性网络百科全书，其通过文章的形式对词条进行多方面的介绍，文章中包含大量的结构化信息，例如文字、信息框模板、分类信息、图片、地理坐标信息、超链接等，这些多模态的信息能丰富知识的多样性，并且建立知识的关联。但作为网络应用，Wikipedia的搜索能力和其他网络应用一样，只能满足关键词的搜索，这种状况大大的降低了知识之间的关联和价值，同时因为其作为大规模协同性内容编辑平台，文章内容也难以避免的出现数据矛盾、不一致的分类和错误。

Auer等人为了充分挖掘Wikipedia中已有的人类知识，并构建知识结构，提出了DBpedia知识库\citing{auer2007dbpedia}。Wikipedia为实现统一的文章风格，因此在文章编辑中镶嵌了一些信息框模板，如图\ref{dbpedia}。
\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\textwidth]{dbpedia.png}
	\caption{Wikipedia的信息框模板和加载效果}
	\label{dbpedia}
\end{figure}
DBpedia利用信息框提取算法检测信息框模板，并且提取出关键的信息，再将信息转化为资源描述框架（RDF）的三元组结构，从而将Wikipedia的文章内容转化为机器可读的结构化信息。最初版本的DBpedia知识库包含关于195万实体的信息，实体内容包括人物、地点、音乐专辑和电影，除了实体外还包含65.7万个图片链接、160万个外部网页链接、18万个其他资源描述框架（RDF）数据库、20.7万个Wikipedia目录和7.5万个YAGO类别\citing{suchanek2007yago}。随着开放社区的数据丰富，2016年推出的版本中已经包含6600万实体，实体的类型扩充了视频、游戏、组织、物种和疾病\citing{wikipedia2016}。资源描述框架的三元组数据量也从1亿增长到130亿之多。

为了增强DBpedia的数据易用性，Auer等人提供了三种数据获取方式：链接数据、SPARQL协议和可下载的RDF文件。链接数据通过HTTP协议获取发布与互联网上的RDF数据，提供给语义网络浏览器、语义网路爬虫和语义网络查询客户端访问\citing{timlinked}。SPARQL是专门针对资源描述框架的查询语言，通过SPARQL终端向\url{http://dbpedia.org/sparql}发送查询指令，DBpedia知识库会返回相应的查询结果。可下载的RDF文件包含序列化的RDF三元组数据，DBpedia将整个数据库按照数据的类型分为众多子数据集，例如，文章目录集、目录标签集、地理坐标集、图像集等。

知识库的内容多样性、易用性和大体量为DBpedia应用提供了良好的基础设施，因此一些自然语言问答和交互的应用都选择建立在DBpedia丰富的知识之上。NLI-GO DBpedia是一个针对通用自然语言交互的应用程序，程序可以接受自然语言问题，并通过SPARQL查询DBpedia知识库，给出答案，实际上这就是基于DBpedia的文本问答系统\citing{nli-go}，类似的还有款基于DBpedia的聊天机器人——DBpedia Chatbot。许多基于知识库的视觉问答研究也选择了数据更加准确的DBpedia\citing{wang2015explicit,wang2017fvqa,wu2016ask}。

\textbf{OpenIE}
应用于构建知识库的信息提取技术（IR）往往需要人为构建大量手写规则，并选择合适的语料库，当已有的提取模型面对全新领域的语料库时，需要重新编写提取规则或者标注数据，这种系统在面对快速迭代和具有丰富多样性的互联网数据时，便会遇到自动化程度低、语料库异质性和效率问题。

为了节省信息提取过程的自动化程度，并能大范围应用于不同领域，Banko等人提出了一种能自主学习不同语料库的信息提取模型——开放信息提取技术（Open IE）\citing{banko2007open}。Open IE以语料库为输入，通过内部算法对语料库中的语句进行一次遍历，最终提取出语句中蕴含的（实体，关系，实体）三元组数据，在整个过程中不需要人工参与，因此可以应用于不同领域知识库的构建。

Banko等人还提出了一种应用高扩展性Open IE模型的系统TEXTRUNNER。TEXTRUNNER由自监督学习器、单通道提取器、基于冗余的评估器三个主要模块构成。自监督学习器以小的语料样本作为训练集，首先使用语句解析器从样本中粗略地提取出（实体，关系，实体）的三元组数据，再对提取出的内容进行标注，标注为“可信”和“不可信”两种标签，将带有标签的数据作为朴素贝叶斯分类器的训练样本。提取器遍历整个语料库，提取出所有可能的三元组数据。对于同一个句子，提取器能生成一个或多个三元组数据，这些数据将被送入学习器训练得到的分类器中，保留所有分在“可信”类别的数据。在得到所有提取出的知识后，评估器融合相同的数据，计算不同的数据的数量。基于以上统计，评估器对每一个三元组数据分配一个用于判断知识正确性的概率值，其中的假设是，如果从多个的语句中提取出相同的知识，那么该知识拥有较高的可信度。

在实验阶段，TEXTRUNNER从包含1.3亿个句子的900万个网页中提取出6000万个三元组数据，平均每个句子提取出2.2个关系数据。通过数据过滤、随机抽取、人工判定等方式，作者对提取数据的完整性和正确性进行了概率评估，过滤后的数据包含1130万个三元组数据，其中780万的数据被评估为“格式正确”且概率标签在0.8以上，80.4\%“格式正确”的数据通过人工评估被认定为正确的，从实体间的关系看，“格式正确”的数据中反映抽象事实的占86\%，其中77.2\%是正确的；反映具体事实的占14\%，其中88.1\%是正确的，如图所示\ref{textrunner}。
\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{textrunner.png}
	\caption{TEXTRUNNER在实验环境下知识提取的正确率}
	\label{textrunner}
\end{figure}

Wu等人在TEXTRUNNER的基础上提出了WOE开放信息提取系统\citing{wu2010open}。WOE改进了自监督学习方式用于构建提取器，TEXTRUNNER在提取过程中使用解析器直接从语料库中提取（实体，关系，实体）的三元组数据，而WOE则先从Wikipedia的信息框中提取“属性-值”对，再使用匹配器从文章中找到包含文章主语和“属性-值”对的句子作为语料库中的训练数据。随后测试了两种解析方法的提取器：WOE-parse和WOE-pos，WOE-pos使用和TEXTRUNNER类似的解析方法，根据简单的词性标签，从语料库中的句子解析出（实体，关系，实体）的数据，WOE-parse则选择更复杂的依赖解析树，希望能再复杂长句的解析中得到更好的精确度。

开放信息提取系统会对每个输出的三元组数据给定一个置信度，如果给定一个置信度的下限，高置信度的数据被保留，低置信度的数据被过滤，此时可以通过精确度和召回率测试系统的性能。精确度是指保留的数据中正确的数据所占比例，能反映整体精确度的平均水平。召回率是指保留的数据中正确的数据占所有正确数据的比例，能反映正确的数据在不同置信度的分布情况。

实验分析显示，因为使用了更友好的训练数据，WOE-pos在精确度上更优于TEXTRUNNER，而WOE-parse在解析树的帮助下实现了最好的性能，特别是在召回率上。

Fader等人在分析TEXTRUNNER和WOE的结果之后发现，不连贯提取和无信息提取两种错误频繁出现。不连贯提取是指被提取的关系语句由多词组成，但语义不连贯而无意义。无信息提取是指提取内容忽略了句子的关键信息，例如，“父亲对母亲做出承诺”，系统返回无信息的（父亲，做出，承诺）而不是（父亲，做出承诺对，母亲）。以上的两种错误都是由系统不能提取出具有完整句法结构的关系语句造成的，Fader等人在Open IE系统中引入了一定的句法限制，提出了REVERB开放信息提取系统\citing{fader2011identifying}。30\%的REVERB提取数据的概率标签在0.8或更高，相较起TEXTRUNNER的0.13\%，在精确度上实现了越阶式的增长，不连贯提取和无信息提取的错误率也大幅减少。

\textbf{Freebase}
Bollacker等人试图结合一般数据库的扩展和Wikipedia等百科全书的多样性，提出了Freebase数据库\citing{bollacker2008freebase}。Freebase和其他常用的知识库相同，使用资源描述框架的三元组形式结构化真实世界的知识，但同时继承了网络百科全书的开放和协同的思想，所有的内容创造和维护都由社区成员协作完成。Freebase存储的元组数据超过1亿2500万条，超过4000种类型和7000种属性，允许使用查询语言通过HTTP协议获取数据。

2014年，Google宣布关停Freebase并将数据迁移至Wikidata。

\textbf{Wikidata}
Wikidata是为了更高效地开放使用和管理Wikipedia文章中数据而提出的协同知识库\citing{vrandevcic2014wikidata}。由于Wikidata的出发点是希望通过大规模协同的方式构建知识库，因此Wikidata的数据具有开放性、多版本共存、多语言、易用性和持续更新的特性。Wikidata向所有用户提供数据扩展和编辑的权限；Wikidata为保证模糊数据的存疑性，相互之间有冲突的数据被同时展示；考虑到数字、日期、坐标等语言无关的数据内容，Wikidata与Wikipedia相同设计为多语言版本；Wikidata数据被组织成Json、RDF的形式发布于网络，通过网络服务能够轻松获取数据；社区成员的持续更新能保持Wikidata的时效性。

Wikidata数据的基本单元被称为项目（Item），每个项目包含名称标签、“Q+数字“的项目编码、描述、别名、和声明。声明中包含一系列属性和相应的值，用于详细描述项目的特点，项目页面如图\ref{wiki-item}。
\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{wiki-item.png}
	\caption{wikidata项目页面}
	\label{wiki-item}
\end{figure}
项目之间通过有向无环图的方式构成，节点代表项目，有向线段代表项目之间的关系，如图\ref{wiki-dag}。截止到2018年，Wikidata已拥有超过5000万个项目。
\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{wiki-dag.png}
	\caption{wikidata项目之间的有向无环图结构}
	\label{wiki-dag}
\end{figure}

Wikidata于2012年提出，相较起以往的知识库，开放性更强，限制也更少。对比YAGO和DBpedia，Wikidata不是从Wikipedia的目录或者信息框中提取信息，相反Wikidata被社区成员独立构建，并为Wikipedia作为知识源，数据被链接到Wikipedia文章中。对比Freebase将对象按类型划分的方式，Wikidata支持对所有对象赋予任意属性。

\section{本文的主要贡献与创新}
本论文以时域积分方程时间步进算法的数值实现技术、后时稳定性问题以及两层平面波加速算法为重点研究内容，主要创新点与贡献如下：

\section{本论文的结构安排}
本文的章节结构安排如下：
